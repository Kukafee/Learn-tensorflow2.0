{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import multiprocessing\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "# scipy1.3.0  中没有 scipy.misc.toimage 方法，scipy1.0.0 中有，可以安装指定版本\n",
    "from scipy.misc import toimage  # scipy.misc 中缺少toimage方法，用PIL中的Image.fromarray代替\n",
    "from PIL import Image \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukafee/environments/tf2_py3/lib/python3.6/site-packages/tensorflow/python/client/session.py:1717: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "config = ConfigProto()   \n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "tf.random.set_seed(22)\n",
    "np.random.seed(22)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "assert tf.__version__.startswith('2.')\n",
    "assert np.__version__.startswith('1.16.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义生成器G   向量[b, 100] => 图片[b, 81, 81, 3]\n",
    "class Generator(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # 输入一个向量，经过生成器，变成一个图片，图片逐渐变大，通道数不断减少\n",
    "        # z:[b, 100] => [b, 3*3*512]  全连接层        \n",
    "        self.fc = layers.Dense(3*3*512)\n",
    "        \n",
    "        # [b, 3, 3, 512] => [b, 9, 9, 256]   反卷积层(输出通道数_卷积核数量；核的大小；步长；padding)\n",
    "        # Conv2DTranspose 是考虑什么样的feature_map通过这个卷积层变成了现在的feature_map\n",
    "        self.conv1 = layers.Conv2DTranspose(256, 3, 3, 'valid')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        # [b, 9, 9, 256] => [b, 29, 29, 128]  此处不是[b, 27, 27, 128]可能是因为受卷积核大小的影响吧\n",
    "        self.conv2 = layers.Conv2DTranspose(128, 5, 3, 'valid')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        # [b, 29, 29, 128] => [b, 88, 88, 3]\n",
    "        self.conv3 = layers.Conv2DTranspose(3, 4, 3, 'valid')\n",
    "        \n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        # [z, 100] => [z, 3*3*512]\n",
    "        x = self.fc(inputs)\n",
    "#         print(x.shape)\n",
    "        \n",
    "        # 获得图片feature_map\n",
    "        x = tf.reshape(x, [-1, 3, 3, 512])        \n",
    "        # 通过leaky_relu() 函数\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "#         print(x.shape)\n",
    "        \n",
    "        # [b, 3, 3, 512] => [b, 9, 9, 256]\n",
    "        x = tf.nn.leaky_relu(self.bn1(self.conv1(x), training=training))\n",
    "#         print(x.shape)\n",
    "        \n",
    "        # [b, 9, 9, 256] => [b, 29, 29, 128]  因为卷积核比较大，是(5*5)所以得到是29*29\n",
    "        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training))\n",
    "#         print(x.shape)\n",
    "        \n",
    "        # [b, 29, 29, 128] => [b, 88, 88, 3]\n",
    "        x = self.conv3(x)\n",
    "        x = tf.tanh(x)\n",
    "        \n",
    "        return x    # [b, 88, 88, 3]\n",
    "    \n",
    "    \n",
    "# 定义判别器D  图片[b, 81, 81, 3] => 向量[b, 1]\n",
    "class Discriminator(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # [b, 81, 81, 3] => [b, 27, 27, 64]\n",
    "        self.conv1 = layers.Conv2D(64, 5, 3, 'valid')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        # [b, 27, 27, 64] => [b, 9, 9, 128]\n",
    "        self.conv2 = layers.Conv2D(128, 5, 3, 'valid')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        # [b, 9, 9, 128] => [b, 3, 3, 256]\n",
    "        self.conv3 = layers.Conv2D(256, 5, 3, 'valid')\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        \n",
    "        # [b, 3, 3, 256] => [b, -1]\n",
    "        # 调用 layers.Flatten() 函数\n",
    "        self.flatten = layers.Flatten()\n",
    "        # 通过一个全连接层，输出节点数为1\n",
    "        self.fc = layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        x = tf.nn.leaky_relu(self.bn1(self.conv1(inputs), training=training))\n",
    "        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training))\n",
    "        x = tf.nn.leaky_relu(self.bn3(self.conv3(x), training=training))\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        logits = self.fc(x)\n",
    "        \n",
    "        return logits    # [b, 1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 定义测试函数 \n",
    "# def main():\n",
    "#     d_net = Discriminator()\n",
    "#     g_net = Generator()\n",
    "    \n",
    "#     x = tf.random.normal([2, 96, 96, 3])\n",
    "#     z = tf.random.normal([2, 200])\n",
    "#     x_prob = d_net(x)\n",
    "#     x_hat = g_net(z)\n",
    "#     print(x_prob)\n",
    "#     print(x_hat.shape)\n",
    "\n",
    "# # 调用测试函数\n",
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存图片的函数save_result()\n",
    "# save_result(fake_image.numpy(), 10, img_path, color_mode='P')    # [100, 88, 88, 3] 在 -1 到 1 之间\n",
    "def save_result(val_out, val_block_size, image_path, color_mode):\n",
    "    def preprocess(img):\n",
    "        img = ((img + 1.0) * 127.5).astype(np.uint8)    # 变换到（0， 255），再转换数据类型为np.uint8\n",
    "        # img = img.astype(np.uint8)\n",
    "        return img\n",
    "\n",
    "    preprocesed = preprocess(val_out)    # 获得经过preprocess()处理过的图片数据，在（0,255）之间     \n",
    "    single_row = np.array([])     # 定义一行\n",
    "    final_image = np.array([])    # 每次添加 single_row 一行的数据\n",
    "    for b in range(val_out.shape[0]):    # val_out.shape[0] = 100,一张一张处理图片；b = 0, 1, ... 99\n",
    "        # concat image into a row\n",
    "        if single_row.size == 0:\n",
    "            single_row = preprocesed[b, :, :, :] # preprocessed[0]表示第b张图片，并添加到single_row数组\n",
    "        else:\n",
    "            single_row = np.concatenate((single_row, preprocesed[b, :, :, :]), axis=1)\n",
    "\n",
    "        # concat image row to final_image\n",
    "        if (b+1) % val_block_size == 0:   # 每val_block_size（此处是10）张截取一行，当b=9时执行\n",
    "            if final_image.size == 0:    \n",
    "                final_image = single_row    # 把 single_row 中的数据拷贝至 final_image\n",
    "            else:    # axis=0 表示 每次添加一行图片数据\n",
    "                final_image = np.concatenate((final_image, single_row), axis=0)\n",
    "\n",
    "            # reset single row\n",
    "            single_row = np.array([])\n",
    "\n",
    "    if final_image.shape[2] == 1:    # 防止 final_image 出现 [10, 10, 1]情况，squeeze 高维度的1\n",
    "        final_image = np.squeeze(final_image, axis=2)\n",
    "    \n",
    "    toimage(final_image).save(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 make_anime_dataset()  用于获取anime数据集中的每一张图片\n",
    "def make_anime_dataset(img_paths, batch_size, resize=88, drop_remainder=True, shuffle=True, repeat=1):\n",
    "    @tf.function\n",
    "    def _map_fn(img):\n",
    "        img = tf.image.resize(img, [resize, resize])\n",
    "        img = tf.clip_by_value(img, 0, 255)\n",
    "        img = img / 127.5 - 1\n",
    "        return img\n",
    "\n",
    "    dataset = disk_image_batch_dataset(img_paths,\n",
    "                                          batch_size,\n",
    "                                          drop_remainder=drop_remainder,\n",
    "                                          map_fn=_map_fn,\n",
    "                                          shuffle=shuffle,\n",
    "                                          repeat=repeat)\n",
    "    img_shape = (resize, resize, 3)\n",
    "    len_dataset = len(img_paths) // batch_size\n",
    "\n",
    "    return dataset, img_shape, len_dataset\n",
    "\n",
    "\n",
    "def batch_dataset(dataset,\n",
    "                  batch_size,\n",
    "                  drop_remainder=True,\n",
    "                  n_prefetch_batch=1,\n",
    "                  filter_fn=None,\n",
    "                  map_fn=None,\n",
    "                  n_map_threads=None,\n",
    "                  filter_after_map=False,\n",
    "                  shuffle=True,\n",
    "                  shuffle_buffer_size=None,\n",
    "                  repeat=None):\n",
    "    # set defaults\n",
    "    if n_map_threads is None:\n",
    "        n_map_threads = multiprocessing.cpu_count()\n",
    "    if shuffle and shuffle_buffer_size is None:\n",
    "        shuffle_buffer_size = max(batch_size * 128, 2048)  # set the minimum buffer size as 2048\n",
    "\n",
    "    # [*] it is efficient to conduct `shuffle` before `map`/`filter` because `map`/`filter` is sometimes costly\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "\n",
    "    if not filter_after_map:\n",
    "        if filter_fn:\n",
    "            dataset = dataset.filter(filter_fn)\n",
    "\n",
    "        if map_fn:\n",
    "            dataset = dataset.map(map_fn, num_parallel_calls=n_map_threads)\n",
    "\n",
    "    else:  # [*] this is slower\n",
    "        if map_fn:\n",
    "            dataset = dataset.map(map_fn, num_parallel_calls=n_map_threads)\n",
    "\n",
    "        if filter_fn:\n",
    "            dataset = dataset.filter(filter_fn)\n",
    "\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n",
    "\n",
    "    dataset = dataset.repeat(repeat).prefetch(n_prefetch_batch)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def memory_data_batch_dataset(memory_data,\n",
    "                              batch_size,\n",
    "                              drop_remainder=True,\n",
    "                              n_prefetch_batch=1,\n",
    "                              filter_fn=None,\n",
    "                              map_fn=None,\n",
    "                              n_map_threads=None,\n",
    "                              filter_after_map=False,\n",
    "                              shuffle=True,\n",
    "                              shuffle_buffer_size=None,\n",
    "                              repeat=None):\n",
    "    \"\"\"Batch dataset of memory data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    memory_data : nested structure of tensors/ndarrays/lists\n",
    "\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(memory_data)\n",
    "    dataset = batch_dataset(dataset,\n",
    "                            batch_size,\n",
    "                            drop_remainder=drop_remainder,\n",
    "                            n_prefetch_batch=n_prefetch_batch,\n",
    "                            filter_fn=filter_fn,\n",
    "                            map_fn=map_fn,\n",
    "                            n_map_threads=n_map_threads,\n",
    "                            filter_after_map=filter_after_map,\n",
    "                            shuffle=shuffle,\n",
    "                            shuffle_buffer_size=shuffle_buffer_size,\n",
    "                            repeat=repeat)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def disk_image_batch_dataset(img_paths,\n",
    "                             batch_size,\n",
    "                             labels=None,\n",
    "                             drop_remainder=True,\n",
    "                             n_prefetch_batch=1,\n",
    "                             filter_fn=None,\n",
    "                             map_fn=None,\n",
    "                             n_map_threads=None,\n",
    "                             filter_after_map=False,\n",
    "                             shuffle=True,\n",
    "                             shuffle_buffer_size=None,\n",
    "                             repeat=None):\n",
    "    \"\"\"Batch dataset of disk image for PNG and JPEG.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        img_paths : 1d-tensor/ndarray/list of str\n",
    "        labels : nested structure of tensors/ndarrays/lists\n",
    "\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        memory_data = img_paths\n",
    "    else:\n",
    "        memory_data = (img_paths, labels)\n",
    "\n",
    "    def parse_fn(path, *label):\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_png(img, 3)  # fix channels to 3\n",
    "        return (img,) + label\n",
    "\n",
    "    if map_fn:  # fuse `map_fn` and `parse_fn`\n",
    "        def map_fn_(*args):\n",
    "            return map_fn(*parse_fn(*args))\n",
    "    else:\n",
    "        map_fn_ = parse_fn\n",
    "\n",
    "    dataset = memory_data_batch_dataset(memory_data,\n",
    "                                        batch_size,\n",
    "                                        drop_remainder=drop_remainder,\n",
    "                                        n_prefetch_batch=n_prefetch_batch,\n",
    "                                        filter_fn=filter_fn,\n",
    "                                        map_fn=map_fn_,\n",
    "                                        n_map_threads=n_map_threads,\n",
    "                                        filter_after_map=filter_after_map,\n",
    "                                        shuffle=shuffle,\n",
    "                                        shuffle_buffer_size=shuffle_buffer_size,\n",
    "                                        repeat=repeat)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 loss_ones()函数，logits 与 [1, 1, 1, ..., 1] 之间的误差\n",
    "def loss_ones(logits):\n",
    "    # [b, 1]  VS. [b] = [1, 1, ... , 1]\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=tf.ones_like(logits))\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# 定义 loss_zeros()函数，logits 与 [0, 0, 0, ..., 0] 之间的误差\n",
    "def loss_zeros(logits):\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=tf.zeros_like(logits))\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# 定义计算判别器discriminator的损失函数\n",
    "# d_loss = d_loss_fn(generator, discriminator, batch_z, batch_x, is_training)\n",
    "def d_loss_fn(generator, discriminator, batch_z, batch_x, is_training):\n",
    "    # treat real image as real 对于真实的图片数据\n",
    "    d_real_logits = discriminator(batch_x, is_training)\n",
    "    # treat generated image as fake 对于生成的图片数据\n",
    "    d_fake_logits = discriminator(generator(batch_z, is_training), is_training)\n",
    "    # 计算真实图片的损失\n",
    "    d_loss_real = loss_ones(d_real_logits)\n",
    "    # 计算生成图片的损失（fake）\n",
    "    d_loss_fake = loss_zeros(d_fake_logits)\n",
    "    loss = d_loss_real + d_loss_fake\n",
    "    return loss\n",
    "\n",
    "# 定义计算生成器generator的损失函数\n",
    "# g_loss = g_loss_fn(generator, discriminator, batch_z, is_training):\n",
    "def g_loss_fn(generator, discriminator, batch_z, is_training):\n",
    "    d_fake_logits = discriminator(generator(batch_z, is_training), is_training)\n",
    "    g_loss_fake = loss_ones(d_fake_logits)    \n",
    "    return g_loss_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义输入到生成器的向量维度\n",
    "z_dim = 100    # z[b, 100] => x_hat[b, 88, 88, 3] => x_hat_pro[b, 1]\n",
    "# 定义训练世代数\n",
    "epochs = 3000000  # 三百万次\n",
    "# epochs = 200\n",
    "# 定义批处理数据大小\n",
    "batch_size = 1024\n",
    "# 定义学习率\n",
    "learning_rate = 2e-3\n",
    "# 定义训练参数\n",
    "is_training = True\n",
    "# 定义数据集中图片的路径\n",
    "#返回所有匹配的文件路径列表。它只有一个参数pathname，定义了文件路径匹配规则\n",
    "img_path = glob.glob('/home/kukafee/shared/faces/*.jpg')\n",
    "# 获取数据集中每批次的数据\n",
    "dataset, img_shape, _ = make_anime_dataset(img_path, batch_size)\n",
    "# print(dataset, img_shape) # <PrefetchDataset shapes: (512, 88, 88, 3), types: tf.float32> (88, 88, 3)\n",
    "# 构建迭代器，并查看其中的元素\n",
    "# sample = next(iter(dataset))\n",
    "# print(sample.shape, tf.reduce_max(sample).numpy(), tf.reduce_min(sample).numpy())\n",
    "# 使得可迭代对象可循环\n",
    "dataset = dataset.repeat()\n",
    "db_iter = iter(dataset)    # 构建迭代器 db_iter\n",
    "\n",
    "# 定义并构建生成器网络\n",
    "generator = Generator()\n",
    "generator.build(input_shape = (None, z_dim))\n",
    "# generator.summary()\n",
    "# 定义并构建判别器网络\n",
    "discriminator = Discriminator()\n",
    "discriminator.build(input_shape = (None, 88, 88, 3))\n",
    "# discriminator.summary()\n",
    "# 定义优化器 \n",
    "# 其中参数 beta_1: A float value or a constant float tensor. The exponential decay rate for the 1st\n",
    "# moment estimates.\n",
    "g_optimizer = tf.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5)\n",
    "d_optimizer = tf.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练并测试GAN\n",
    "def main():\n",
    "    # 循环每一个世代\n",
    "    for epoch in range(epochs):\n",
    "        # 定义一批要送入生成器的随机均匀分布数据\n",
    "        # z[b, 100] => x_hat[b, 88, 88, 3] => x_hat_pro[b, 1]\n",
    "        batch_z = tf.random.uniform([batch_size, z_dim], minval=-1., maxval=1.)\n",
    "        # 从迭代器 db_iter 中生成要送入判别器的图片数据\n",
    "        batch_x = next(db_iter)\n",
    "        for i in range(2):            \n",
    "            # 训练判别器D——discriminator\n",
    "            with tf.GradientTape() as tape:\n",
    "                d_loss = d_loss_fn(generator, discriminator, batch_z, batch_x, is_training)\n",
    "            # 计算梯度\n",
    "            grads = tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "            # 更新参数\n",
    "            d_optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
    "                \n",
    "        # 训练生成器G——generator\n",
    "        with tf.GradientTape() as tape:\n",
    "            g_loss = g_loss_fn(generator, discriminator, batch_z, is_training)\n",
    "        grads = tape.gradient(g_loss, generator.trainable_variables)\n",
    "        g_optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n",
    "        \n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(epoch, 'd-loss: ', float(d_loss), 'g-loss: ', float(g_loss))\n",
    "            \n",
    "            # 测试生气器网络效果\n",
    "            # 从正态分布中随机产生数据\n",
    "            z = tf.random.uniform([100, z_dim])    # [100, 100]\n",
    "            fake_image = generator(z, training=False)    # [100, 88, 88, 3]\n",
    "            # '/home/kukafee/workspace/picture/GAN/fake_epoch_%d.png'%epoch\n",
    "            img_path = os.path.join('/home/kukafee/workspace/picture/GAN', 'fake_epoch_%d.png'%epoch)\n",
    "            save_result(fake_image.numpy(), 10, img_path, color_mode='P')\n",
    "            \n",
    "        if epoch % 1000 == 0:            \n",
    "            # 保存 network 的******参数******到文件\n",
    "            generator.save_weights('/home/kukafee/workspace/save_model/GAN/G_weight_%d.ckpt'%epoch)\n",
    "            # 打印信息\n",
    "            print('Saved G_weights: %d'%epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 d-loss:  0.0718667209148407 g-loss:  7.487253665924072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukafee/environments/tf2_py3/lib/python3.6/site-packages/ipykernel_launcher.py:32: DeprecationWarning: `toimage` is deprecated!\n",
      "`toimage` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use Pillow's ``Image.fromarray`` directly instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved G_weights: 0\n",
      "100 d-loss:  0.6367347240447998 g-loss:  2.079136610031128\n",
      "200 d-loss:  0.5834982395172119 g-loss:  2.646066665649414\n",
      "300 d-loss:  0.7264952659606934 g-loss:  2.5978095531463623\n",
      "400 d-loss:  0.7830707430839539 g-loss:  2.7816150188446045\n",
      "500 d-loss:  0.5083626508712769 g-loss:  2.8121726512908936\n",
      "600 d-loss:  0.4349428713321686 g-loss:  3.2510011196136475\n",
      "700 d-loss:  0.8292799592018127 g-loss:  4.630350112915039\n",
      "800 d-loss:  0.39947476983070374 g-loss:  4.245325088500977\n",
      "900 d-loss:  0.4732692837715149 g-loss:  3.1950910091400146\n",
      "1000 d-loss:  0.22153708338737488 g-loss:  4.5294718742370605\n",
      "Saved G_weights: 1000\n",
      "1100 d-loss:  0.3207417130470276 g-loss:  4.110607624053955\n",
      "1200 d-loss:  0.3428756892681122 g-loss:  4.444893836975098\n",
      "1300 d-loss:  0.6931564807891846 g-loss:  3.2388126850128174\n",
      "1400 d-loss:  0.2429182231426239 g-loss:  4.16689395904541\n",
      "1500 d-loss:  0.2755606174468994 g-loss:  3.3392763137817383\n",
      "1600 d-loss:  0.47679081559181213 g-loss:  3.067136287689209\n",
      "1700 d-loss:  0.21606740355491638 g-loss:  3.708693265914917\n",
      "1800 d-loss:  0.3676512539386749 g-loss:  3.7594876289367676\n",
      "1900 d-loss:  0.29552561044692993 g-loss:  3.841764211654663\n",
      "2000 d-loss:  0.27541840076446533 g-loss:  3.6322340965270996\n",
      "Saved G_weights: 2000\n",
      "2100 d-loss:  0.23493923246860504 g-loss:  3.745142936706543\n",
      "2200 d-loss:  0.2852042019367218 g-loss:  3.3317434787750244\n",
      "2300 d-loss:  0.30298158526420593 g-loss:  3.369767427444458\n",
      "2400 d-loss:  0.22530992329120636 g-loss:  3.5833446979522705\n",
      "2500 d-loss:  0.39020323753356934 g-loss:  3.9161782264709473\n",
      "2600 d-loss:  0.30603915452957153 g-loss:  3.8534228801727295\n",
      "2700 d-loss:  0.41224709153175354 g-loss:  3.312061309814453\n",
      "2800 d-loss:  0.39401260018348694 g-loss:  3.634927988052368\n",
      "2900 d-loss:  1.3333755731582642 g-loss:  0.9552410244941711\n",
      "3000 d-loss:  0.2467695027589798 g-loss:  3.61537504196167\n",
      "Saved G_weights: 3000\n",
      "3100 d-loss:  0.3294897973537445 g-loss:  4.566952705383301\n",
      "3200 d-loss:  0.22918538749217987 g-loss:  3.8486149311065674\n",
      "3300 d-loss:  0.38745442032814026 g-loss:  3.6254825592041016\n",
      "3400 d-loss:  0.32856467366218567 g-loss:  3.3917059898376465\n",
      "3500 d-loss:  0.2038211226463318 g-loss:  3.6287732124328613\n",
      "3600 d-loss:  0.16644103825092316 g-loss:  4.007264137268066\n",
      "3700 d-loss:  0.31344571709632874 g-loss:  3.244473695755005\n",
      "3800 d-loss:  0.3051077723503113 g-loss:  3.415147304534912\n",
      "3900 d-loss:  0.31150710582733154 g-loss:  4.466117858886719\n",
      "4000 d-loss:  0.30044007301330566 g-loss:  4.188658714294434\n",
      "Saved G_weights: 4000\n",
      "4100 d-loss:  0.3540515899658203 g-loss:  3.270463228225708\n",
      "4200 d-loss:  1.4335246086120605 g-loss:  1.8429722785949707\n",
      "4300 d-loss:  0.1375916749238968 g-loss:  4.72733211517334\n",
      "4400 d-loss:  0.3175112307071686 g-loss:  3.0128567218780518\n",
      "4500 d-loss:  0.6607082486152649 g-loss:  3.539047956466675\n",
      "4600 d-loss:  0.40642911195755005 g-loss:  3.5601119995117188\n",
      "4700 d-loss:  0.24062050879001617 g-loss:  3.755068302154541\n",
      "4800 d-loss:  0.786607563495636 g-loss:  1.9227194786071777\n",
      "4900 d-loss:  0.4801536500453949 g-loss:  2.8619155883789062\n",
      "5000 d-loss:  0.32583653926849365 g-loss:  4.7088823318481445\n",
      "Saved G_weights: 5000\n",
      "5100 d-loss:  0.1926795393228531 g-loss:  3.7156078815460205\n",
      "5200 d-loss:  0.1727433055639267 g-loss:  4.010837554931641\n",
      "5300 d-loss:  0.14126214385032654 g-loss:  4.601953029632568\n",
      "5400 d-loss:  2.063357353210449 g-loss:  0.43483030796051025\n",
      "5500 d-loss:  0.20011083781719208 g-loss:  3.5107421875\n",
      "5600 d-loss:  0.8958476781845093 g-loss:  1.9486452341079712\n",
      "5700 d-loss:  0.1885075569152832 g-loss:  4.049215793609619\n",
      "5800 d-loss:  0.16242331266403198 g-loss:  3.7876386642456055\n",
      "5900 d-loss:  0.17759475111961365 g-loss:  3.7827255725860596\n",
      "6000 d-loss:  0.5969766974449158 g-loss:  2.858396530151367\n",
      "Saved G_weights: 6000\n",
      "6100 d-loss:  0.7458590269088745 g-loss:  3.3447728157043457\n",
      "6200 d-loss:  0.308616578578949 g-loss:  3.9882290363311768\n",
      "6300 d-loss:  0.1635866016149521 g-loss:  4.145225524902344\n",
      "6400 d-loss:  0.058169275522232056 g-loss:  5.181202411651611\n",
      "6500 d-loss:  0.3441627621650696 g-loss:  3.5339789390563965\n",
      "6600 d-loss:  0.04333871230483055 g-loss:  6.164778709411621\n",
      "6700 d-loss:  1.074756383895874 g-loss:  1.361069917678833\n",
      "6800 d-loss:  0.1310502588748932 g-loss:  4.656546592712402\n",
      "6900 d-loss:  0.558596670627594 g-loss:  3.433757781982422\n",
      "7000 d-loss:  0.8461006283760071 g-loss:  2.734344482421875\n",
      "Saved G_weights: 7000\n",
      "7100 d-loss:  0.9187772274017334 g-loss:  1.6976613998413086\n",
      "7200 d-loss:  0.17240671813488007 g-loss:  4.632136821746826\n",
      "7300 d-loss:  0.03562796860933304 g-loss:  5.626870632171631\n",
      "7400 d-loss:  0.17696836590766907 g-loss:  4.088864803314209\n",
      "7500 d-loss:  0.1967684030532837 g-loss:  4.143027305603027\n",
      "7600 d-loss:  0.17489251494407654 g-loss:  3.7703475952148438\n",
      "7700 d-loss:  0.556585431098938 g-loss:  1.3226288557052612\n",
      "7800 d-loss:  0.13330486416816711 g-loss:  4.583743095397949\n",
      "7900 d-loss:  0.12476977705955505 g-loss:  4.485456466674805\n",
      "8000 d-loss:  0.691784143447876 g-loss:  4.926535606384277\n",
      "Saved G_weights: 8000\n",
      "8100 d-loss:  0.06414669752120972 g-loss:  5.220011234283447\n",
      "8200 d-loss:  0.11888504773378372 g-loss:  4.520730018615723\n",
      "8300 d-loss:  0.1043027937412262 g-loss:  4.567326068878174\n",
      "8400 d-loss:  0.22555753588676453 g-loss:  5.4266486167907715\n",
      "8500 d-loss:  0.04774758219718933 g-loss:  5.710511207580566\n",
      "8600 d-loss:  0.05511649698019028 g-loss:  5.260965824127197\n",
      "8700 d-loss:  0.20788079500198364 g-loss:  3.9782049655914307\n",
      "8800 d-loss:  0.13793572783470154 g-loss:  4.562728404998779\n",
      "8900 d-loss:  0.11422628164291382 g-loss:  4.859185695648193\n",
      "9000 d-loss:  1.4215376377105713 g-loss:  2.632631301879883\n",
      "Saved G_weights: 9000\n",
      "9100 d-loss:  0.17993268370628357 g-loss:  4.6527299880981445\n",
      "9200 d-loss:  0.12181995809078217 g-loss:  4.651301383972168\n",
      "9300 d-loss:  0.024080747738480568 g-loss:  6.375925540924072\n",
      "9400 d-loss:  0.142387256026268 g-loss:  3.4771289825439453\n",
      "9500 d-loss:  0.160744771361351 g-loss:  4.538261413574219\n",
      "9600 d-loss:  0.5757293701171875 g-loss:  3.728806257247925\n",
      "9700 d-loss:  0.12452256679534912 g-loss:  4.150981426239014\n",
      "9800 d-loss:  0.034400574862957 g-loss:  5.326390743255615\n",
      "9900 d-loss:  0.09379785507917404 g-loss:  5.062623977661133\n",
      "10000 d-loss:  0.033102378249168396 g-loss:  5.886552333831787\n",
      "Saved G_weights: 10000\n",
      "10100 d-loss:  0.029698777943849564 g-loss:  5.938331604003906\n",
      "10200 d-loss:  0.08285246044397354 g-loss:  4.998159885406494\n",
      "10300 d-loss:  0.056248221546411514 g-loss:  5.113669395446777\n",
      "10400 d-loss:  0.5037257671356201 g-loss:  4.607695579528809\n",
      "10500 d-loss:  1.1184601783752441 g-loss:  2.761491060256958\n",
      "10600 d-loss:  0.02858622558414936 g-loss:  6.4616827964782715\n",
      "10700 d-loss:  0.07600711286067963 g-loss:  5.243041038513184\n",
      "10800 d-loss:  0.13206061720848083 g-loss:  4.9117512702941895\n",
      "10900 d-loss:  0.5261565446853638 g-loss:  3.2444982528686523\n",
      "11000 d-loss:  0.1841513067483902 g-loss:  4.806405067443848\n",
      "Saved G_weights: 11000\n",
      "11100 d-loss:  0.06385467946529388 g-loss:  5.463265419006348\n",
      "11200 d-loss:  0.42199310660362244 g-loss:  2.1747195720672607\n",
      "11300 d-loss:  0.1048445850610733 g-loss:  5.0330071449279785\n",
      "11400 d-loss:  0.16668157279491425 g-loss:  6.768840312957764\n",
      "11500 d-loss:  0.057307854294776917 g-loss:  5.479284286499023\n",
      "11600 d-loss:  0.04669416695833206 g-loss:  5.592737197875977\n",
      "11700 d-loss:  0.0295469481498003 g-loss:  6.475100517272949\n",
      "11800 d-loss:  0.022249963134527206 g-loss:  6.993292808532715\n",
      "11900 d-loss:  0.07821578532457352 g-loss:  5.536319255828857\n",
      "12000 d-loss:  0.10475295782089233 g-loss:  5.026872634887695\n",
      "Saved G_weights: 12000\n",
      "12100 d-loss:  0.14227184653282166 g-loss:  5.064143180847168\n",
      "12200 d-loss:  0.027445781975984573 g-loss:  6.772369861602783\n",
      "12300 d-loss:  0.01386426854878664 g-loss:  7.3589653968811035\n",
      "12400 d-loss:  0.11570209264755249 g-loss:  4.9763994216918945\n",
      "12500 d-loss:  0.0366571806371212 g-loss:  6.698165416717529\n",
      "12600 d-loss:  0.07539492100477219 g-loss:  5.764749526977539\n",
      "12700 d-loss:  0.09160568565130234 g-loss:  5.27822732925415\n",
      "12800 d-loss:  0.03868897259235382 g-loss:  6.212265968322754\n",
      "12900 d-loss:  0.06545805931091309 g-loss:  5.645155429840088\n",
      "13000 d-loss:  0.08282708376646042 g-loss:  5.132449150085449\n",
      "Saved G_weights: 13000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13100 d-loss:  0.036004118621349335 g-loss:  6.073843955993652\n",
      "13200 d-loss:  0.09015777707099915 g-loss:  5.097859859466553\n",
      "13300 d-loss:  0.06839781999588013 g-loss:  5.648865699768066\n",
      "13400 d-loss:  0.07355491071939468 g-loss:  5.790332317352295\n",
      "13500 d-loss:  0.0756712257862091 g-loss:  5.076382160186768\n",
      "13600 d-loss:  0.02178790420293808 g-loss:  6.775018692016602\n",
      "13700 d-loss:  0.18581831455230713 g-loss:  5.417026996612549\n",
      "13800 d-loss:  0.09198113530874252 g-loss:  5.086434364318848\n",
      "13900 d-loss:  0.12090808153152466 g-loss:  5.623741626739502\n",
      "14000 d-loss:  0.14621379971504211 g-loss:  4.867206573486328\n",
      "Saved G_weights: 14000\n",
      "14100 d-loss:  0.07356808334589005 g-loss:  5.450902462005615\n",
      "14200 d-loss:  1.3403098583221436 g-loss:  4.235217094421387\n",
      "14300 d-loss:  0.06046878546476364 g-loss:  5.802402973175049\n",
      "14400 d-loss:  0.021769458428025246 g-loss:  6.602114200592041\n",
      "14500 d-loss:  0.07790430635213852 g-loss:  5.629189968109131\n",
      "14600 d-loss:  0.07975228875875473 g-loss:  5.829830169677734\n",
      "14700 d-loss:  0.05193021520972252 g-loss:  5.741624355316162\n",
      "14800 d-loss:  0.1977093368768692 g-loss:  4.175288677215576\n",
      "14900 d-loss:  0.35162612795829773 g-loss:  4.2681884765625\n",
      "15000 d-loss:  0.07810205966234207 g-loss:  5.714666843414307\n",
      "Saved G_weights: 15000\n",
      "15100 d-loss:  0.06034764647483826 g-loss:  5.82647180557251\n",
      "15200 d-loss:  0.07279007136821747 g-loss:  5.753276348114014\n",
      "15300 d-loss:  0.05030396580696106 g-loss:  5.833836555480957\n",
      "15400 d-loss:  0.049033887684345245 g-loss:  6.0230865478515625\n",
      "15500 d-loss:  1.5389169454574585 g-loss:  0.981906533241272\n",
      "15600 d-loss:  0.058367133140563965 g-loss:  6.438783168792725\n",
      "15700 d-loss:  0.08135784417390823 g-loss:  5.566511154174805\n",
      "15800 d-loss:  1.3200048208236694 g-loss:  3.1787338256835938\n",
      "15900 d-loss:  0.051535096019506454 g-loss:  6.029850006103516\n",
      "16000 d-loss:  0.10256140679121017 g-loss:  5.055838584899902\n",
      "Saved G_weights: 16000\n",
      "16100 d-loss:  0.08269466459751129 g-loss:  5.880784034729004\n",
      "16200 d-loss:  0.054982736706733704 g-loss:  5.788778305053711\n",
      "16300 d-loss:  0.043478623032569885 g-loss:  6.046392440795898\n",
      "16400 d-loss:  0.09109383821487427 g-loss:  5.58143949508667\n",
      "16500 d-loss:  0.5867505669593811 g-loss:  3.3902266025543213\n",
      "16600 d-loss:  0.03710796684026718 g-loss:  6.165542125701904\n",
      "16700 d-loss:  0.042783576995134354 g-loss:  6.085908889770508\n",
      "16800 d-loss:  0.04821785166859627 g-loss:  6.074398040771484\n",
      "16900 d-loss:  0.06545330584049225 g-loss:  6.276852130889893\n",
      "17000 d-loss:  0.04606444388628006 g-loss:  6.137136936187744\n",
      "Saved G_weights: 17000\n",
      "17100 d-loss:  0.03748875856399536 g-loss:  6.358541965484619\n",
      "17200 d-loss:  0.04833398759365082 g-loss:  6.271969795227051\n",
      "17300 d-loss:  0.0772324725985527 g-loss:  5.500470161437988\n",
      "17400 d-loss:  0.037734586745500565 g-loss:  6.604747772216797\n",
      "17500 d-loss:  0.047136105597019196 g-loss:  5.837971210479736\n",
      "17600 d-loss:  0.014571819454431534 g-loss:  7.362372398376465\n",
      "17700 d-loss:  0.025334816426038742 g-loss:  7.414401531219482\n",
      "17800 d-loss:  0.018206246197223663 g-loss:  7.386775970458984\n",
      "17900 d-loss:  0.04518713802099228 g-loss:  6.1315999031066895\n",
      "18000 d-loss:  0.0520278736948967 g-loss:  6.117021083831787\n",
      "Saved G_weights: 18000\n",
      "18100 d-loss:  0.014248223975300789 g-loss:  7.380162239074707\n",
      "18200 d-loss:  0.34332242608070374 g-loss:  4.262338161468506\n",
      "18300 d-loss:  0.08385571837425232 g-loss:  5.931686878204346\n",
      "18400 d-loss:  0.035200025886297226 g-loss:  6.9398112297058105\n",
      "18500 d-loss:  0.05914079025387764 g-loss:  5.637560844421387\n",
      "18600 d-loss:  0.15377385914325714 g-loss:  4.929840087890625\n",
      "18700 d-loss:  0.052695419639348984 g-loss:  6.673624038696289\n",
      "18800 d-loss:  0.0498347133398056 g-loss:  6.401795387268066\n",
      "18900 d-loss:  0.2309444397687912 g-loss:  4.2199015617370605\n",
      "19000 d-loss:  0.056826576590538025 g-loss:  5.984112739562988\n",
      "Saved G_weights: 19000\n",
      "19100 d-loss:  0.017968973144888878 g-loss:  7.573392391204834\n",
      "19200 d-loss:  0.07196414470672607 g-loss:  5.89479398727417\n",
      "19300 d-loss:  0.0799146294593811 g-loss:  5.531858444213867\n",
      "19400 d-loss:  0.03955551236867905 g-loss:  6.181501865386963\n",
      "19500 d-loss:  0.0646963119506836 g-loss:  6.370152950286865\n",
      "19600 d-loss:  0.3832149803638458 g-loss:  5.357504844665527\n",
      "19700 d-loss:  0.05578742176294327 g-loss:  5.599761009216309\n",
      "19800 d-loss:  0.03523099422454834 g-loss:  6.305079460144043\n",
      "19900 d-loss:  0.42617297172546387 g-loss:  4.050812244415283\n",
      "20000 d-loss:  0.3741106390953064 g-loss:  5.218875408172607\n",
      "Saved G_weights: 20000\n",
      "20100 d-loss:  0.04452431946992874 g-loss:  6.267587661743164\n",
      "20200 d-loss:  0.026501663029193878 g-loss:  6.9364118576049805\n",
      "20300 d-loss:  0.05051269009709358 g-loss:  9.696115493774414\n",
      "20400 d-loss:  0.2787325978279114 g-loss:  5.394314765930176\n",
      "20500 d-loss:  0.06964420527219772 g-loss:  5.5971527099609375\n",
      "20600 d-loss:  0.07892975211143494 g-loss:  5.397177219390869\n",
      "20700 d-loss:  0.8545874357223511 g-loss:  3.00447154045105\n",
      "20800 d-loss:  0.10034526884555817 g-loss:  5.411780834197998\n",
      "20900 d-loss:  0.03407192975282669 g-loss:  6.866372108459473\n",
      "21000 d-loss:  0.034946244210004807 g-loss:  7.3849406242370605\n",
      "Saved G_weights: 21000\n",
      "21100 d-loss:  0.005761901848018169 g-loss:  8.360700607299805\n",
      "21200 d-loss:  0.1588270664215088 g-loss:  5.556831359863281\n",
      "21300 d-loss:  0.10065197199583054 g-loss:  5.075376510620117\n",
      "21400 d-loss:  0.055966705083847046 g-loss:  6.519114017486572\n",
      "21500 d-loss:  0.04910959303379059 g-loss:  6.859597682952881\n",
      "21600 d-loss:  0.052216313779354095 g-loss:  6.034666061401367\n",
      "21700 d-loss:  0.03961553797125816 g-loss:  6.90871524810791\n",
      "21800 d-loss:  0.04851147532463074 g-loss:  6.190814018249512\n",
      "21900 d-loss:  0.060822296887636185 g-loss:  7.302562236785889\n",
      "22000 d-loss:  0.01411665603518486 g-loss:  7.076964378356934\n",
      "Saved G_weights: 22000\n",
      "22100 d-loss:  0.05786100775003433 g-loss:  6.35092306137085\n",
      "22200 d-loss:  0.022451486438512802 g-loss:  7.431405067443848\n",
      "22300 d-loss:  0.03161293640732765 g-loss:  7.651385307312012\n",
      "22400 d-loss:  0.1191537156701088 g-loss:  5.5810675621032715\n",
      "22500 d-loss:  0.06243564933538437 g-loss:  6.052239418029785\n",
      "22600 d-loss:  0.06447216868400574 g-loss:  6.241608619689941\n",
      "22700 d-loss:  0.07959058880805969 g-loss:  5.767348766326904\n",
      "22800 d-loss:  0.07527845352888107 g-loss:  5.984528064727783\n",
      "22900 d-loss:  0.06112493574619293 g-loss:  5.91036319732666\n",
      "23000 d-loss:  0.032894738018512726 g-loss:  7.095016956329346\n",
      "Saved G_weights: 23000\n",
      "23100 d-loss:  0.01141270063817501 g-loss:  7.831968784332275\n",
      "23200 d-loss:  0.05949372425675392 g-loss:  5.963676452636719\n",
      "23300 d-loss:  0.057596586644649506 g-loss:  6.714522361755371\n",
      "23400 d-loss:  0.03174922242760658 g-loss:  6.5955610275268555\n",
      "23500 d-loss:  0.0381617546081543 g-loss:  6.3777923583984375\n",
      "23600 d-loss:  0.03268562629818916 g-loss:  6.383869171142578\n",
      "23700 d-loss:  0.07509392499923706 g-loss:  7.41302490234375\n",
      "23800 d-loss:  0.03635097295045853 g-loss:  6.3869099617004395\n",
      "23900 d-loss:  0.04208638519048691 g-loss:  6.438859939575195\n",
      "24000 d-loss:  0.17909470200538635 g-loss:  5.487969875335693\n",
      "Saved G_weights: 24000\n",
      "24100 d-loss:  0.04291947931051254 g-loss:  6.709609031677246\n",
      "24200 d-loss:  0.06985104829072952 g-loss:  6.513089179992676\n",
      "24300 d-loss:  0.029925521463155746 g-loss:  6.9614715576171875\n",
      "24400 d-loss:  0.03800998628139496 g-loss:  6.722467422485352\n",
      "24500 d-loss:  0.05744858458638191 g-loss:  6.403343677520752\n",
      "24600 d-loss:  0.0683770701289177 g-loss:  7.083843231201172\n",
      "24700 d-loss:  0.09184254705905914 g-loss:  5.65238618850708\n",
      "24800 d-loss:  0.039248496294021606 g-loss:  6.578518867492676\n",
      "24900 d-loss:  0.032578244805336 g-loss:  7.04168176651001\n",
      "25000 d-loss:  0.015348385088145733 g-loss:  8.235546112060547\n",
      "Saved G_weights: 25000\n",
      "25100 d-loss:  0.08888863027095795 g-loss:  7.140956401824951\n",
      "25200 d-loss:  0.23507295548915863 g-loss:  8.548707008361816\n",
      "25300 d-loss:  0.026995321735739708 g-loss:  7.051257133483887\n",
      "25400 d-loss:  0.15122723579406738 g-loss:  6.37612247467041\n",
      "25500 d-loss:  0.04041844978928566 g-loss:  6.620373725891113\n",
      "25600 d-loss:  0.018879160284996033 g-loss:  7.860039234161377\n",
      "25700 d-loss:  0.04695843532681465 g-loss:  6.704297065734863\n",
      "25800 d-loss:  0.249686598777771 g-loss:  8.359397888183594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25900 d-loss:  0.06096625328063965 g-loss:  6.51792049407959\n",
      "26000 d-loss:  0.03932839632034302 g-loss:  6.730444431304932\n",
      "Saved G_weights: 26000\n",
      "26100 d-loss:  0.09954620897769928 g-loss:  6.668071746826172\n",
      "26200 d-loss:  0.043621040880680084 g-loss:  8.662736892700195\n",
      "26300 d-loss:  0.2509566843509674 g-loss:  3.4179444313049316\n",
      "26400 d-loss:  0.135908842086792 g-loss:  6.940043926239014\n",
      "26500 d-loss:  0.1970258355140686 g-loss:  4.284502983093262\n",
      "26600 d-loss:  0.04831460490822792 g-loss:  6.7554707527160645\n",
      "26700 d-loss:  0.006254047155380249 g-loss:  9.270856857299805\n",
      "26800 d-loss:  0.022872203961014748 g-loss:  7.436927318572998\n",
      "26900 d-loss:  0.043473079800605774 g-loss:  7.588587284088135\n",
      "27000 d-loss:  0.9431150555610657 g-loss:  0.7978678941726685\n",
      "Saved G_weights: 27000\n",
      "27100 d-loss:  0.20676526427268982 g-loss:  4.08985710144043\n",
      "27200 d-loss:  0.011709030717611313 g-loss:  8.794455528259277\n",
      "27300 d-loss:  0.04156637191772461 g-loss:  6.9836835861206055\n",
      "27400 d-loss:  0.052556317299604416 g-loss:  6.674875736236572\n",
      "27500 d-loss:  0.02905324101448059 g-loss:  7.380766868591309\n",
      "27600 d-loss:  0.04986576735973358 g-loss:  6.918903827667236\n",
      "27700 d-loss:  0.02882106974720955 g-loss:  7.309823989868164\n",
      "27800 d-loss:  0.03861353546380997 g-loss:  6.967552185058594\n",
      "27900 d-loss:  0.014899816364049911 g-loss:  8.270035743713379\n",
      "28000 d-loss:  0.021945996209979057 g-loss:  7.131078243255615\n",
      "Saved G_weights: 28000\n",
      "28100 d-loss:  0.18305669724941254 g-loss:  6.36541748046875\n",
      "28200 d-loss:  0.08161154389381409 g-loss:  6.14148473739624\n",
      "28300 d-loss:  0.02467864379286766 g-loss:  7.227449417114258\n",
      "28400 d-loss:  0.02348097786307335 g-loss:  7.676225662231445\n",
      "28500 d-loss:  0.008364900015294552 g-loss:  9.26461410522461\n",
      "28600 d-loss:  0.1495136022567749 g-loss:  5.6632490158081055\n",
      "28700 d-loss:  0.06433118134737015 g-loss:  7.219478607177734\n",
      "28800 d-loss:  0.10608584433794022 g-loss:  5.826247692108154\n",
      "28900 d-loss:  0.28045934438705444 g-loss:  5.55025053024292\n",
      "29000 d-loss:  0.18021835386753082 g-loss:  5.147580623626709\n",
      "Saved G_weights: 29000\n",
      "29100 d-loss:  0.04567175731062889 g-loss:  6.717968463897705\n",
      "29200 d-loss:  0.044003862887620926 g-loss:  6.993513107299805\n",
      "29300 d-loss:  0.04642148315906525 g-loss:  6.533935070037842\n",
      "29400 d-loss:  0.025033775717020035 g-loss:  7.360246181488037\n",
      "29500 d-loss:  0.016444604843854904 g-loss:  7.857596397399902\n",
      "29600 d-loss:  0.010404695756733418 g-loss:  7.907097816467285\n",
      "29700 d-loss:  0.052707865834236145 g-loss:  6.9891510009765625\n",
      "29800 d-loss:  0.0681854784488678 g-loss:  6.524749755859375\n",
      "29900 d-loss:  0.34122201800346375 g-loss:  3.479781150817871\n",
      "30000 d-loss:  0.06367456912994385 g-loss:  5.946308135986328\n",
      "Saved G_weights: 30000\n",
      "30100 d-loss:  0.07393908500671387 g-loss:  6.020402908325195\n",
      "30200 d-loss:  0.034134093672037125 g-loss:  7.10333251953125\n",
      "30300 d-loss:  0.019387204200029373 g-loss:  8.230960845947266\n",
      "30400 d-loss:  0.012642879039049149 g-loss:  8.149887084960938\n",
      "30500 d-loss:  0.04004728049039841 g-loss:  6.778744220733643\n",
      "30600 d-loss:  0.04457106441259384 g-loss:  7.1723527908325195\n",
      "30700 d-loss:  0.17698873579502106 g-loss:  7.937868118286133\n",
      "30800 d-loss:  0.05854811146855354 g-loss:  7.041774749755859\n",
      "30900 d-loss:  0.1402735710144043 g-loss:  5.466463088989258\n",
      "31000 d-loss:  0.06684079766273499 g-loss:  7.300452709197998\n",
      "Saved G_weights: 31000\n",
      "31100 d-loss:  0.004151787143200636 g-loss:  9.564496994018555\n",
      "31200 d-loss:  0.05021291226148605 g-loss:  6.650688171386719\n",
      "31300 d-loss:  0.04679613560438156 g-loss:  7.507086277008057\n",
      "31400 d-loss:  0.1293535679578781 g-loss:  7.11122465133667\n",
      "31500 d-loss:  0.055752720683813095 g-loss:  6.726944446563721\n",
      "31600 d-loss:  0.039017628878355026 g-loss:  7.254727363586426\n",
      "31700 d-loss:  0.056823424994945526 g-loss:  6.451649188995361\n",
      "31800 d-loss:  0.030879251658916473 g-loss:  7.37814474105835\n",
      "31900 d-loss:  0.016404911875724792 g-loss:  8.737824440002441\n",
      "32000 d-loss:  0.04076378047466278 g-loss:  6.919425010681152\n",
      "Saved G_weights: 32000\n",
      "32100 d-loss:  0.030375424772500992 g-loss:  8.38615894317627\n",
      "32200 d-loss:  0.5882024765014648 g-loss:  4.563395023345947\n",
      "32300 d-loss:  0.04436030238866806 g-loss:  6.641170024871826\n",
      "32400 d-loss:  0.017414184287190437 g-loss:  8.567424774169922\n",
      "32500 d-loss:  0.017444005236029625 g-loss:  7.964680194854736\n",
      "32600 d-loss:  0.017459966242313385 g-loss:  7.774229049682617\n",
      "32700 d-loss:  0.04502665996551514 g-loss:  7.646078109741211\n",
      "32800 d-loss:  0.6023544669151306 g-loss:  3.715172529220581\n",
      "32900 d-loss:  0.009969601407647133 g-loss:  8.404321670532227\n",
      "33000 d-loss:  0.06446173042058945 g-loss:  7.10502290725708\n",
      "Saved G_weights: 33000\n",
      "33100 d-loss:  0.020195581018924713 g-loss:  7.742362976074219\n",
      "33200 d-loss:  0.032332681119441986 g-loss:  6.84842586517334\n",
      "33300 d-loss:  0.06937576830387115 g-loss:  7.1975274085998535\n",
      "33400 d-loss:  0.03337593749165535 g-loss:  7.008729457855225\n",
      "33500 d-loss:  0.295218288898468 g-loss:  4.506302833557129\n",
      "33600 d-loss:  0.047810573130846024 g-loss:  6.649116516113281\n",
      "33700 d-loss:  0.03289242088794708 g-loss:  7.217361927032471\n",
      "33800 d-loss:  0.02042641118168831 g-loss:  9.087505340576172\n",
      "33900 d-loss:  0.0102556012570858 g-loss:  9.015718460083008\n",
      "34000 d-loss:  0.019665716215968132 g-loss:  7.881917476654053\n",
      "Saved G_weights: 34000\n",
      "34100 d-loss:  0.03560400754213333 g-loss:  7.613713264465332\n",
      "34200 d-loss:  0.041742485016584396 g-loss:  6.819996356964111\n",
      "34300 d-loss:  0.031344834715127945 g-loss:  7.339615821838379\n",
      "34400 d-loss:  0.05692946910858154 g-loss:  6.732367515563965\n",
      "34500 d-loss:  0.04287974536418915 g-loss:  6.833452224731445\n",
      "34600 d-loss:  0.04035775363445282 g-loss:  8.536846160888672\n",
      "34700 d-loss:  0.02650124952197075 g-loss:  7.801613807678223\n",
      "34800 d-loss:  0.029772985726594925 g-loss:  8.32116413116455\n",
      "34900 d-loss:  0.030148105695843697 g-loss:  8.00021743774414\n",
      "35000 d-loss:  0.009466895833611488 g-loss:  8.746749877929688\n",
      "Saved G_weights: 35000\n",
      "35100 d-loss:  0.03093874454498291 g-loss:  8.41091537475586\n",
      "35200 d-loss:  0.041655778884887695 g-loss:  6.840412139892578\n",
      "35300 d-loss:  1.6795181035995483 g-loss:  2.087268829345703\n",
      "35400 d-loss:  0.0202023908495903 g-loss:  7.433659076690674\n",
      "35500 d-loss:  0.03693784028291702 g-loss:  6.942002773284912\n",
      "35600 d-loss:  0.056701868772506714 g-loss:  7.1965789794921875\n",
      "35700 d-loss:  0.10002436488866806 g-loss:  13.516459465026855\n",
      "35800 d-loss:  0.2039140909910202 g-loss:  6.729814052581787\n",
      "35900 d-loss:  0.030295444652438164 g-loss:  7.208295822143555\n",
      "36000 d-loss:  0.03496699035167694 g-loss:  7.354250431060791\n",
      "Saved G_weights: 36000\n",
      "36100 d-loss:  0.021316975355148315 g-loss:  8.155437469482422\n",
      "36200 d-loss:  0.01767994835972786 g-loss:  8.092287063598633\n",
      "36300 d-loss:  0.014103638008236885 g-loss:  10.202731132507324\n",
      "36400 d-loss:  0.21214567124843597 g-loss:  8.249865531921387\n",
      "36500 d-loss:  0.03703577071428299 g-loss:  7.439485549926758\n",
      "36600 d-loss:  0.013698790222406387 g-loss:  7.620710372924805\n",
      "36700 d-loss:  0.02508554980158806 g-loss:  7.972894191741943\n",
      "36800 d-loss:  0.35575637221336365 g-loss:  4.889200687408447\n",
      "36900 d-loss:  0.049960389733314514 g-loss:  7.355501174926758\n",
      "37000 d-loss:  0.015284626744687557 g-loss:  8.830677032470703\n",
      "Saved G_weights: 37000\n",
      "37100 d-loss:  0.04027462378144264 g-loss:  6.830852031707764\n",
      "37200 d-loss:  0.013903968967497349 g-loss:  8.802444458007812\n",
      "37300 d-loss:  0.09158153831958771 g-loss:  8.77348804473877\n",
      "37400 d-loss:  0.023866748437285423 g-loss:  8.24649715423584\n",
      "37500 d-loss:  0.03629255294799805 g-loss:  8.076356887817383\n",
      "37600 d-loss:  0.07816524803638458 g-loss:  7.8728790283203125\n",
      "37700 d-loss:  0.01397537812590599 g-loss:  9.172024726867676\n",
      "37800 d-loss:  0.02567567303776741 g-loss:  7.536470890045166\n",
      "37900 d-loss:  0.021885769441723824 g-loss:  9.722305297851562\n",
      "38000 d-loss:  0.0020809017587453127 g-loss:  14.424015998840332\n",
      "Saved G_weights: 38000\n",
      "38100 d-loss:  0.023050136864185333 g-loss:  49.23575210571289\n",
      "38200 d-loss:  0.002391704823821783 g-loss:  32.29655456542969\n",
      "38300 d-loss:  0.042822565883398056 g-loss:  51.42936325073242\n",
      "38400 d-loss:  0.00875919871032238 g-loss:  42.206912994384766\n",
      "38500 d-loss:  0.025895263999700546 g-loss:  100.57508850097656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38600 d-loss:  0.006598150357604027 g-loss:  55.96088409423828\n",
      "38700 d-loss:  0.009899331256747246 g-loss:  75.1407241821289\n",
      "38800 d-loss:  0.012187689542770386 g-loss:  52.62348175048828\n",
      "38900 d-loss:  0.008153031580150127 g-loss:  23.02196502685547\n",
      "39000 d-loss:  0.003551672911271453 g-loss:  91.9942398071289\n",
      "Saved G_weights: 39000\n",
      "39100 d-loss:  0.0020620382856577635 g-loss:  51.28694152832031\n",
      "39200 d-loss:  0.14496588706970215 g-loss:  112.18732452392578\n",
      "39300 d-loss:  0.007240184582769871 g-loss:  38.30497741699219\n",
      "39400 d-loss:  0.00659925676882267 g-loss:  25.879751205444336\n",
      "39500 d-loss:  0.017151718959212303 g-loss:  51.992088317871094\n",
      "39600 d-loss:  0.0001442656066501513 g-loss:  17.09652328491211\n",
      "39700 d-loss:  0.001181091065518558 g-loss:  34.69975280761719\n",
      "39800 d-loss:  0.00307950540445745 g-loss:  14.423110961914062\n",
      "39900 d-loss:  0.0002196468849433586 g-loss:  26.975914001464844\n",
      "40000 d-loss:  0.000203285992029123 g-loss:  22.316749572753906\n",
      "Saved G_weights: 40000\n",
      "40100 d-loss:  0.0004174640926066786 g-loss:  19.832271575927734\n",
      "40200 d-loss:  4.5316355681279674e-05 g-loss:  20.142908096313477\n",
      "40300 d-loss:  0.0002139789576176554 g-loss:  54.517799377441406\n",
      "40400 d-loss:  0.00048773596063256264 g-loss:  44.178951263427734\n",
      "40500 d-loss:  0.0010805511847138405 g-loss:  64.06537628173828\n",
      "40600 d-loss:  0.004591657314449549 g-loss:  171.69644165039062\n",
      "40700 d-loss:  0.00024761405074968934 g-loss:  20.411483764648438\n",
      "40800 d-loss:  0.002828366821631789 g-loss:  174.96365356445312\n",
      "40900 d-loss:  2.2066822111810325e-06 g-loss:  76.85673522949219\n",
      "41000 d-loss:  0.00028596248012036085 g-loss:  11.171390533447266\n",
      "Saved G_weights: 41000\n",
      "41100 d-loss:  8.284676368930377e-06 g-loss:  83.04484558105469\n",
      "41200 d-loss:  8.795764188107569e-06 g-loss:  22.43082046508789\n",
      "41300 d-loss:  0.00010076538455905393 g-loss:  48.10682678222656\n",
      "41400 d-loss:  3.5965746064903215e-05 g-loss:  17.29694366455078\n",
      "41500 d-loss:  3.561169796739705e-05 g-loss:  44.019737243652344\n",
      "41600 d-loss:  0.0006743876729160547 g-loss:  151.56910705566406\n",
      "41700 d-loss:  0.00012286321725696325 g-loss:  128.5280303955078\n",
      "41800 d-loss:  0.02035573683679104 g-loss:  230.87396240234375\n",
      "41900 d-loss:  7.709122291998938e-05 g-loss:  22.234527587890625\n",
      "42000 d-loss:  0.00035866396501660347 g-loss:  10.667379379272461\n",
      "Saved G_weights: 42000\n",
      "42100 d-loss:  3.3270644053118303e-05 g-loss:  33.05113983154297\n",
      "42200 d-loss:  0.0005468743038363755 g-loss:  23.779537200927734\n",
      "42300 d-loss:  0.0003481627209112048 g-loss:  89.0858383178711\n",
      "42400 d-loss:  6.28703783149831e-05 g-loss:  46.69353485107422\n",
      "42500 d-loss:  1.7601079889573157e-05 g-loss:  18.010391235351562\n",
      "42600 d-loss:  3.687683420139365e-05 g-loss:  14.363125801086426\n",
      "42700 d-loss:  0.0003255169722251594 g-loss:  11.227518081665039\n",
      "42800 d-loss:  4.3243660911684856e-05 g-loss:  18.599393844604492\n",
      "42900 d-loss:  3.592385837691836e-05 g-loss:  28.874189376831055\n",
      "43000 d-loss:  3.4281219996046275e-05 g-loss:  111.70120239257812\n",
      "Saved G_weights: 43000\n",
      "43100 d-loss:  3.1695570214651525e-05 g-loss:  15.193246841430664\n",
      "43200 d-loss:  1.828326094255317e-05 g-loss:  36.442222595214844\n",
      "43300 d-loss:  1.6150563624250935e-06 g-loss:  70.69332122802734\n",
      "43400 d-loss:  3.0345478080562316e-05 g-loss:  29.682388305664062\n",
      "43500 d-loss:  6.653107220699894e-07 g-loss:  30.930376052856445\n",
      "43600 d-loss:  0.022597094997763634 g-loss:  225.93255615234375\n",
      "43700 d-loss:  1.0027479220298119e-05 g-loss:  12.967048645019531\n",
      "43800 d-loss:  2.272985875606537e-05 g-loss:  17.02838134765625\n",
      "43900 d-loss:  2.824393959599547e-05 g-loss:  11.600260734558105\n",
      "44000 d-loss:  4.370176975498907e-05 g-loss:  20.336170196533203\n",
      "Saved G_weights: 44000\n",
      "44100 d-loss:  2.9783112040604465e-05 g-loss:  12.88159465789795\n",
      "44200 d-loss:  7.284249932126841e-06 g-loss:  20.06705093383789\n",
      "44300 d-loss:  6.334568286581188e-13 g-loss:  103.60481262207031\n",
      "44400 d-loss:  6.126874702028218e-11 g-loss:  58.314552307128906\n",
      "44500 d-loss:  0.00024670155835337937 g-loss:  115.68251037597656\n",
      "44600 d-loss:  0.00012850167695432901 g-loss:  91.98758697509766\n",
      "44700 d-loss:  7.703083974774927e-05 g-loss:  34.65422058105469\n",
      "44800 d-loss:  9.276554192183539e-05 g-loss:  29.786014556884766\n",
      "44900 d-loss:  8.94155164132826e-06 g-loss:  17.31905174255371\n",
      "45000 d-loss:  2.6460602384759113e-05 g-loss:  15.114409446716309\n",
      "Saved G_weights: 45000\n",
      "45100 d-loss:  0.00025058985920622945 g-loss:  46.29315948486328\n",
      "45200 d-loss:  9.078084258362651e-05 g-loss:  17.644758224487305\n",
      "45300 d-loss:  2.2936868845135905e-05 g-loss:  27.792083740234375\n",
      "45400 d-loss:  4.417858235683525e-06 g-loss:  38.77784729003906\n",
      "45500 d-loss:  3.343724529258907e-05 g-loss:  33.410499572753906\n",
      "45600 d-loss:  3.6951729271095246e-05 g-loss:  11.981761932373047\n",
      "45700 d-loss:  0.00010219997056992725 g-loss:  17.857267379760742\n",
      "45800 d-loss:  0.00011765272938646376 g-loss:  36.719276428222656\n",
      "45900 d-loss:  1.562492616358213e-05 g-loss:  13.300447463989258\n",
      "46000 d-loss:  4.4061551307095215e-05 g-loss:  33.329307556152344\n",
      "Saved G_weights: 46000\n",
      "46100 d-loss:  4.9904872867045924e-05 g-loss:  16.524763107299805\n",
      "46200 d-loss:  5.81777494517155e-05 g-loss:  19.523128509521484\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE ---- Variational Auto Encoders  变分自动编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在隐藏层，有两类节点：一类是所有特征的均值节点（Mean）；一类是所有特征的方差节点（Variance）。\n",
    "VAE是一个生成模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(22)\n",
    "np.random.seed(22)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEV'] = '2'\n",
    "assert tf.__version__.startswith('2.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(imgs, name):\n",
    "    new_im = Image.new('L', (280, 280))\n",
    "    index = 0\n",
    "    for i in range(0, 280, 28):\n",
    "        for j in range(0, 280, 28):\n",
    "            im = imgs[index]\n",
    "            im = Image.fromarray(im, mode='L')\n",
    "            new_im.paste(im, (i, j))\n",
    "            index += 1\n",
    "            \n",
    "    new_im.save(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过VAE 将图片降维后的维度\n",
    "h_dim = 20\n",
    "batchsz = 10000\n",
    "lr = 1e-3\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "x_train, x_test = x_train.astype(np.float32) / 255., x_test.astype(np.float32) / 255.\n",
    "\n",
    "train_db = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_db = train_db.shuffle(batchsz * 5).batch(batchsz, drop_remainder=True)\n",
    "test_db = tf.data.Dataset.from_tensor_slices(x_test)\n",
    "test_db = test_db.batch(batchsz, drop_remainder=True)\n",
    "\n",
    "z_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "class VAE(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        # 不添加relu激活函数\n",
    "        self.fc1 = layers.Dense(256)    # [b, 28*28] => [b, 256]\n",
    "        self.fc2 = layers.Dense(128)    # [b, 256] => [b, 128]\n",
    "        self.fc_mean = layers.Dense(z_dim)    # fc2:[b, 128] => fc_mean:[b, z_dim]\n",
    "        self.fc_var = layers.Dense(z_dim)     # fc2:[b, 128] => fc_var:[b, z_dim]\n",
    "        \n",
    "        # Decoder\n",
    "#         self.fcd1 = layers.Dence(z_dim)    # [b, z_dim] => [b, z_dim]\n",
    "#         self.fcd2 = layers.Dence(z_dim)    # [b, z_dim] => [b, z_dim]\n",
    "        self.fc3 = layers.Dense(128)    # z_reparameterization:[b, z_dim] => fc3: [b, 128]\n",
    "        self.fc4 = layers.Dense(256)    # fc3:[b, 128] => fc4:[b, 256]\n",
    "        self.fc5 = layers.Dense(28*28)    # fc4:[b, 256] => fc5:[b, 28*28]\n",
    "        \n",
    "    # 创建 encoder 的传播过程\n",
    "    def encoder(self, x):\n",
    "        h1 = tf.nn.relu(self.fc1(x))    # [b, 28*28] => [b, 256]\n",
    "        h2 = tf.nn.relu(self.fc2(h1))   # [b, 256] => [b, 128]\n",
    "        # get mean\n",
    "        h_mean = self.fc_mean(h2)    # [b, 128] => [b, z_dim=10]\n",
    "        # get variance （一般情况下需要取对数，使其在整个实数R上取值）\n",
    "        h_var = self.fc_var(h2)      # [b, 128] => [b, z_dim=10]\n",
    "        \n",
    "        return h_mean, h_var\n",
    "    \n",
    "    # 创建 decoder 的传播过程\n",
    "    def decoder(self, z):\n",
    "        out1 = tf.nn.relu(self.fc3(z))\n",
    "        out2 = tf.nn.relu(self.fc4(out1))\n",
    "        out3 = self.fc5(out2)\n",
    "        \n",
    "        return out3\n",
    "    \n",
    "    # reparameterization trick 再参数化 通过再参数化之后再前向传播为decoder部分\n",
    "    def reparameterization(self, h_mean, h_var):\n",
    "        # h_mean: [b, z_dim]\n",
    "        # h_var:  [b, z_dim]\n",
    "        \n",
    "        # 从标准正态分布中采样的到 eps，此处由 tf.random.normal()，安装 h_var 的形状生成\n",
    "        eps = tf.random.normal(h_var.shape)\n",
    "        # 因为在使用reparameterization时，使用的是标准差，此处由方差计算标准差\n",
    "        # 【关键点】\n",
    "        # 此处的均值是实际意义的均值；但是方差不是实际情况中的方差\n",
    "        # 此处迫使EncodeNetwork 能够学习到方差的对数（以e为底的对数）\n",
    "        # 所以实际意义的方差 var = tf.exp(h_var)\n",
    "#         std = tf.exp(h_var) ** 0.5    # 不应该是这样的\n",
    "        std = tf.exp(h_var) ** 0.5\n",
    "        # 再参数化技巧，实现可导，便于梯度反向传播更新参数\n",
    "        z_reparameterization = h_mean + std * eps\n",
    "        \n",
    "        return z_reparameterization\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        # [b, 28*28] => [b, z_dim], [b, z_dim]\n",
    "        h_mean, h_var = self.encoder(inputs)\n",
    "        # reparameterization trick\n",
    "        z_raparameterization = self.reparameterization(h_mean, h_var)\n",
    "        # 重建 x_hat\n",
    "        x_hat = self.decoder(z_raparameterization)\n",
    "        \n",
    "        return x_hat, h_mean, h_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  200960    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  1290      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  1290      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  1408      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  33024     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  201488    \n",
      "=================================================================\n",
      "Total params: 472,356\n",
      "Trainable params: 472,356\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VAE_model = VAE()\n",
    "VAE_model.build(input_shape=(batchsz, 28*28))    # 为什么不能是（None， 28*28）\n",
    "optimizer = tf.optimizers.Adam(lr=lr)\n",
    "VAE_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Rec loss:  510.4544372558594 KL_div:  2.8124639987945557\n",
      "1 Rec loss:  432.5567932128906 KL_div:  10.553927421569824\n",
      "2 Rec loss:  388.7998352050781 KL_div:  7.959600925445557\n",
      "3 Rec loss:  366.1690368652344 KL_div:  6.751097679138184\n",
      "4 Rec loss:  344.0827941894531 KL_div:  7.7710466384887695\n",
      "5 Rec loss:  327.2944641113281 KL_div:  8.801284790039062\n",
      "6 Rec loss:  314.537353515625 KL_div:  9.207307815551758\n",
      "7 Rec loss:  306.978515625 KL_div:  9.217245101928711\n",
      "8 Rec loss:  299.57366943359375 KL_div:  9.099848747253418\n",
      "9 Rec loss:  295.4237976074219 KL_div:  9.484129905700684\n",
      "10 Rec loss:  291.18084716796875 KL_div:  10.0331449508667\n",
      "11 Rec loss:  287.4630126953125 KL_div:  10.387138366699219\n",
      "12 Rec loss:  284.28131103515625 KL_div:  10.56123161315918\n",
      "13 Rec loss:  280.41412353515625 KL_div:  10.880800247192383\n",
      "14 Rec loss:  277.9190368652344 KL_div:  11.023004531860352\n",
      "15 Rec loss:  274.6007995605469 KL_div:  11.360089302062988\n",
      "16 Rec loss:  271.7486877441406 KL_div:  11.670470237731934\n",
      "17 Rec loss:  270.3453674316406 KL_div:  11.895729064941406\n",
      "18 Rec loss:  267.2039794921875 KL_div:  11.790165901184082\n",
      "19 Rec loss:  265.1927185058594 KL_div:  11.945473670959473\n",
      "20 Rec loss:  263.2511901855469 KL_div:  12.171799659729004\n",
      "21 Rec loss:  261.8938903808594 KL_div:  12.096404075622559\n",
      "22 Rec loss:  261.510009765625 KL_div:  12.06296443939209\n",
      "23 Rec loss:  258.897216796875 KL_div:  12.271890640258789\n",
      "24 Rec loss:  258.05279541015625 KL_div:  12.517257690429688\n",
      "25 Rec loss:  256.77227783203125 KL_div:  12.400534629821777\n",
      "26 Rec loss:  255.56307983398438 KL_div:  12.391971588134766\n",
      "27 Rec loss:  254.58203125 KL_div:  12.290520668029785\n",
      "28 Rec loss:  253.84922790527344 KL_div:  12.21269702911377\n",
      "29 Rec loss:  253.62277221679688 KL_div:  12.034528732299805\n",
      "30 Rec loss:  252.34414672851562 KL_div:  12.350833892822266\n",
      "31 Rec loss:  252.0017547607422 KL_div:  12.33105754852295\n",
      "32 Rec loss:  251.00035095214844 KL_div:  12.395432472229004\n",
      "33 Rec loss:  250.9080810546875 KL_div:  12.058287620544434\n",
      "34 Rec loss:  250.0969696044922 KL_div:  12.554655075073242\n",
      "35 Rec loss:  250.09507751464844 KL_div:  11.99829387664795\n",
      "36 Rec loss:  249.02047729492188 KL_div:  12.357255935668945\n",
      "37 Rec loss:  248.39598083496094 KL_div:  12.580904006958008\n",
      "38 Rec loss:  248.53343200683594 KL_div:  12.163994789123535\n",
      "39 Rec loss:  247.55506896972656 KL_div:  12.616400718688965\n",
      "40 Rec loss:  247.19869995117188 KL_div:  12.328261375427246\n",
      "41 Rec loss:  247.16419982910156 KL_div:  12.17374324798584\n",
      "42 Rec loss:  246.30819702148438 KL_div:  12.605718612670898\n",
      "43 Rec loss:  246.24655151367188 KL_div:  12.171829223632812\n",
      "44 Rec loss:  245.703369140625 KL_div:  12.728281021118164\n",
      "45 Rec loss:  245.35545349121094 KL_div:  12.690284729003906\n",
      "46 Rec loss:  244.8737030029297 KL_div:  12.524027824401855\n",
      "47 Rec loss:  244.55921936035156 KL_div:  12.558721542358398\n",
      "48 Rec loss:  244.21424865722656 KL_div:  12.58979320526123\n",
      "49 Rec loss:  244.62684631347656 KL_div:  12.860857009887695\n",
      "50 Rec loss:  244.38885498046875 KL_div:  12.181374549865723\n",
      "51 Rec loss:  243.32460021972656 KL_div:  12.645147323608398\n",
      "52 Rec loss:  243.11715698242188 KL_div:  12.453507423400879\n",
      "53 Rec loss:  242.6529541015625 KL_div:  12.769323348999023\n",
      "54 Rec loss:  242.41802978515625 KL_div:  12.830214500427246\n",
      "55 Rec loss:  242.0751190185547 KL_div:  12.654215812683105\n",
      "56 Rec loss:  241.77188110351562 KL_div:  12.82122802734375\n",
      "57 Rec loss:  243.30865478515625 KL_div:  13.06687068939209\n",
      "58 Rec loss:  241.4906768798828 KL_div:  12.562518119812012\n",
      "59 Rec loss:  241.749755859375 KL_div:  12.416389465332031\n",
      "60 Rec loss:  240.96725463867188 KL_div:  12.639827728271484\n",
      "61 Rec loss:  240.54397583007812 KL_div:  12.901369094848633\n",
      "62 Rec loss:  240.60568237304688 KL_div:  12.530698776245117\n",
      "63 Rec loss:  240.15719604492188 KL_div:  12.70376968383789\n",
      "64 Rec loss:  239.8509521484375 KL_div:  12.773335456848145\n",
      "65 Rec loss:  240.94020080566406 KL_div:  12.304512023925781\n",
      "66 Rec loss:  240.458251953125 KL_div:  13.264678001403809\n",
      "67 Rec loss:  240.44552612304688 KL_div:  13.235868453979492\n",
      "68 Rec loss:  239.42735290527344 KL_div:  13.070656776428223\n",
      "69 Rec loss:  239.14877319335938 KL_div:  12.779314041137695\n",
      "70 Rec loss:  238.9386444091797 KL_div:  12.782109260559082\n",
      "71 Rec loss:  238.90139770507812 KL_div:  12.725415229797363\n",
      "72 Rec loss:  238.40994262695312 KL_div:  12.865668296813965\n",
      "73 Rec loss:  238.2333221435547 KL_div:  12.932503700256348\n",
      "74 Rec loss:  238.02944946289062 KL_div:  13.042437553405762\n",
      "75 Rec loss:  237.86497497558594 KL_div:  13.019343376159668\n",
      "76 Rec loss:  237.6980438232422 KL_div:  12.955899238586426\n",
      "77 Rec loss:  237.7396697998047 KL_div:  12.811491966247559\n",
      "78 Rec loss:  237.3031463623047 KL_div:  13.081818580627441\n",
      "79 Rec loss:  237.235595703125 KL_div:  12.888337135314941\n",
      "80 Rec loss:  237.05349731445312 KL_div:  12.980926513671875\n",
      "81 Rec loss:  236.99761962890625 KL_div:  13.224509239196777\n",
      "82 Rec loss:  236.7552490234375 KL_div:  13.179193496704102\n",
      "83 Rec loss:  236.80780029296875 KL_div:  12.875611305236816\n",
      "84 Rec loss:  236.43057250976562 KL_div:  13.076498031616211\n",
      "85 Rec loss:  236.34292602539062 KL_div:  13.037493705749512\n",
      "86 Rec loss:  236.32952880859375 KL_div:  13.442059516906738\n",
      "87 Rec loss:  235.99822998046875 KL_div:  13.218449592590332\n",
      "88 Rec loss:  236.166748046875 KL_div:  13.393226623535156\n",
      "89 Rec loss:  235.84378051757812 KL_div:  13.166096687316895\n",
      "90 Rec loss:  236.0477294921875 KL_div:  12.896733283996582\n",
      "91 Rec loss:  235.56272888183594 KL_div:  13.3038969039917\n",
      "92 Rec loss:  235.63487243652344 KL_div:  13.058738708496094\n",
      "93 Rec loss:  235.53419494628906 KL_div:  12.999342918395996\n",
      "94 Rec loss:  235.45574951171875 KL_div:  12.983945846557617\n",
      "95 Rec loss:  236.0773468017578 KL_div:  12.690126419067383\n",
      "96 Rec loss:  235.248046875 KL_div:  13.022982597351074\n",
      "97 Rec loss:  235.02247619628906 KL_div:  13.2467622756958\n",
      "98 Rec loss:  235.15130615234375 KL_div:  12.960001945495605\n",
      "99 Rec loss:  234.7412567138672 KL_div:  13.420509338378906\n",
      "100 Rec loss:  234.7487030029297 KL_div:  13.328857421875\n",
      "101 Rec loss:  234.80259704589844 KL_div:  12.989617347717285\n",
      "102 Rec loss:  234.63409423828125 KL_div:  13.094382286071777\n",
      "103 Rec loss:  234.54144287109375 KL_div:  13.128013610839844\n",
      "104 Rec loss:  234.33692932128906 KL_div:  13.247526168823242\n",
      "105 Rec loss:  236.11782836914062 KL_div:  14.26795768737793\n",
      "106 Rec loss:  236.20457458496094 KL_div:  12.513008117675781\n",
      "107 Rec loss:  235.56724548339844 KL_div:  12.742754936218262\n",
      "108 Rec loss:  234.76641845703125 KL_div:  12.871540069580078\n",
      "109 Rec loss:  234.4700469970703 KL_div:  12.932754516601562\n",
      "110 Rec loss:  234.31692504882812 KL_div:  12.945220947265625\n",
      "111 Rec loss:  233.99600219726562 KL_div:  13.054540634155273\n",
      "112 Rec loss:  233.72802734375 KL_div:  13.214859008789062\n",
      "113 Rec loss:  233.56195068359375 KL_div:  13.350676536560059\n",
      "114 Rec loss:  233.7667694091797 KL_div:  13.053916931152344\n",
      "115 Rec loss:  233.46084594726562 KL_div:  13.268637657165527\n",
      "116 Rec loss:  233.46507263183594 KL_div:  13.125117301940918\n",
      "117 Rec loss:  233.34896850585938 KL_div:  13.207696914672852\n",
      "118 Rec loss:  233.2017059326172 KL_div:  13.270207405090332\n",
      "119 Rec loss:  233.21730041503906 KL_div:  13.24068546295166\n",
      "120 Rec loss:  232.9874267578125 KL_div:  13.495841026306152\n",
      "121 Rec loss:  233.13467407226562 KL_div:  13.16985034942627\n",
      "122 Rec loss:  233.02267456054688 KL_div:  13.17569351196289\n",
      "123 Rec loss:  232.8153076171875 KL_div:  13.299772262573242\n",
      "124 Rec loss:  232.88055419921875 KL_div:  13.254342079162598\n",
      "125 Rec loss:  232.69607543945312 KL_div:  13.358159065246582\n",
      "126 Rec loss:  232.63519287109375 KL_div:  13.437458038330078\n",
      "127 Rec loss:  232.63282775878906 KL_div:  13.710668563842773\n",
      "128 Rec loss:  232.52220153808594 KL_div:  13.300235748291016\n",
      "129 Rec loss:  232.74462890625 KL_div:  13.125218391418457\n",
      "130 Rec loss:  232.31820678710938 KL_div:  13.463018417358398\n",
      "131 Rec loss:  232.22265625 KL_div:  13.490021705627441\n",
      "132 Rec loss:  232.2667999267578 KL_div:  13.387408256530762\n",
      "133 Rec loss:  232.76800537109375 KL_div:  13.038651466369629\n",
      "134 Rec loss:  234.98922729492188 KL_div:  12.567886352539062\n",
      "135 Rec loss:  232.37448120117188 KL_div:  13.251615524291992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136 Rec loss:  232.16722106933594 KL_div:  13.673529624938965\n",
      "137 Rec loss:  231.90660095214844 KL_div:  13.491768836975098\n",
      "138 Rec loss:  232.44842529296875 KL_div:  13.099771499633789\n",
      "139 Rec loss:  231.6725311279297 KL_div:  13.64323616027832\n",
      "140 Rec loss:  231.62158203125 KL_div:  13.650008201599121\n",
      "141 Rec loss:  231.8282928466797 KL_div:  13.29979419708252\n",
      "142 Rec loss:  231.4832000732422 KL_div:  13.643506050109863\n",
      "143 Rec loss:  231.77005004882812 KL_div:  13.247669219970703\n",
      "144 Rec loss:  231.3741455078125 KL_div:  13.627256393432617\n",
      "145 Rec loss:  231.39112854003906 KL_div:  13.458590507507324\n",
      "146 Rec loss:  231.4821014404297 KL_div:  13.341267585754395\n",
      "147 Rec loss:  231.44305419921875 KL_div:  13.287522315979004\n",
      "148 Rec loss:  231.26068115234375 KL_div:  13.665559768676758\n",
      "149 Rec loss:  231.10902404785156 KL_div:  13.63379955291748\n",
      "150 Rec loss:  231.0372772216797 KL_div:  13.680044174194336\n",
      "151 Rec loss:  231.03579711914062 KL_div:  13.91466236114502\n",
      "152 Rec loss:  231.0714569091797 KL_div:  13.509159088134766\n",
      "153 Rec loss:  231.309326171875 KL_div:  13.285354614257812\n",
      "154 Rec loss:  231.3719024658203 KL_div:  13.217874526977539\n",
      "155 Rec loss:  230.84942626953125 KL_div:  13.533540725708008\n",
      "156 Rec loss:  230.73719787597656 KL_div:  13.708593368530273\n",
      "157 Rec loss:  231.0372772216797 KL_div:  14.125893592834473\n",
      "158 Rec loss:  231.39987182617188 KL_div:  13.182690620422363\n",
      "159 Rec loss:  230.60276794433594 KL_div:  13.80783462524414\n",
      "160 Rec loss:  230.89508056640625 KL_div:  13.408602714538574\n",
      "161 Rec loss:  230.55322265625 KL_div:  13.514457702636719\n",
      "162 Rec loss:  230.6220245361328 KL_div:  13.457293510437012\n",
      "163 Rec loss:  230.86325073242188 KL_div:  13.268284797668457\n",
      "164 Rec loss:  230.37661743164062 KL_div:  13.616215705871582\n",
      "165 Rec loss:  230.47337341308594 KL_div:  13.502824783325195\n",
      "166 Rec loss:  231.64300537109375 KL_div:  13.102612495422363\n",
      "167 Rec loss:  230.35662841796875 KL_div:  13.703315734863281\n",
      "168 Rec loss:  230.1510772705078 KL_div:  13.589618682861328\n",
      "169 Rec loss:  230.09970092773438 KL_div:  13.72501277923584\n",
      "170 Rec loss:  230.31430053710938 KL_div:  13.473272323608398\n",
      "171 Rec loss:  230.14627075195312 KL_div:  13.510458946228027\n",
      "172 Rec loss:  229.92825317382812 KL_div:  13.730040550231934\n",
      "173 Rec loss:  229.92356872558594 KL_div:  13.692147254943848\n",
      "174 Rec loss:  229.93345642089844 KL_div:  14.002623558044434\n",
      "175 Rec loss:  229.95440673828125 KL_div:  14.240388870239258\n",
      "176 Rec loss:  229.93194580078125 KL_div:  13.622374534606934\n",
      "177 Rec loss:  230.04193115234375 KL_div:  13.521106719970703\n",
      "178 Rec loss:  229.757080078125 KL_div:  13.844799995422363\n",
      "179 Rec loss:  230.05612182617188 KL_div:  13.454544067382812\n",
      "180 Rec loss:  229.7592010498047 KL_div:  13.59913158416748\n",
      "181 Rec loss:  229.72727966308594 KL_div:  13.613922119140625\n",
      "182 Rec loss:  229.65370178222656 KL_div:  13.68698787689209\n",
      "183 Rec loss:  229.445068359375 KL_div:  13.919371604919434\n",
      "184 Rec loss:  229.56735229492188 KL_div:  13.731232643127441\n",
      "185 Rec loss:  230.00660705566406 KL_div:  13.394556045532227\n",
      "186 Rec loss:  229.53611755371094 KL_div:  13.620266914367676\n",
      "187 Rec loss:  229.4199981689453 KL_div:  13.741720199584961\n",
      "188 Rec loss:  229.55215454101562 KL_div:  13.518189430236816\n",
      "189 Rec loss:  229.58030700683594 KL_div:  13.626221656799316\n",
      "190 Rec loss:  229.33804321289062 KL_div:  13.672932624816895\n",
      "191 Rec loss:  229.19485473632812 KL_div:  13.87273120880127\n",
      "192 Rec loss:  229.9907684326172 KL_div:  14.455663681030273\n",
      "193 Rec loss:  230.49337768554688 KL_div:  13.16690444946289\n",
      "194 Rec loss:  229.0594940185547 KL_div:  14.008929252624512\n",
      "195 Rec loss:  229.0749053955078 KL_div:  13.774734497070312\n",
      "196 Rec loss:  229.41064453125 KL_div:  13.483840942382812\n",
      "197 Rec loss:  228.97254943847656 KL_div:  13.899406433105469\n",
      "198 Rec loss:  229.16360473632812 KL_div:  13.67010498046875\n",
      "199 Rec loss:  229.03897094726562 KL_div:  13.713693618774414\n",
      "200 Rec loss:  228.97372436523438 KL_div:  13.76099967956543\n",
      "201 Rec loss:  229.0607452392578 KL_div:  13.61467170715332\n",
      "202 Rec loss:  228.9042205810547 KL_div:  14.04031753540039\n",
      "203 Rec loss:  229.02975463867188 KL_div:  13.645782470703125\n",
      "204 Rec loss:  229.1197509765625 KL_div:  13.518595695495605\n",
      "205 Rec loss:  228.87625122070312 KL_div:  13.670881271362305\n",
      "206 Rec loss:  228.54698181152344 KL_div:  13.972518920898438\n",
      "207 Rec loss:  228.73497009277344 KL_div:  13.7420654296875\n",
      "208 Rec loss:  228.66819763183594 KL_div:  13.761898040771484\n",
      "209 Rec loss:  228.5285186767578 KL_div:  14.120609283447266\n",
      "210 Rec loss:  228.65261840820312 KL_div:  14.192140579223633\n",
      "211 Rec loss:  229.2428741455078 KL_div:  13.47858715057373\n",
      "212 Rec loss:  228.51502990722656 KL_div:  13.870240211486816\n",
      "213 Rec loss:  228.80796813964844 KL_div:  13.569337844848633\n",
      "214 Rec loss:  228.68553161621094 KL_div:  13.645272254943848\n",
      "215 Rec loss:  228.50094604492188 KL_div:  13.836441040039062\n",
      "216 Rec loss:  228.3754425048828 KL_div:  13.963268280029297\n",
      "217 Rec loss:  228.5708770751953 KL_div:  13.728923797607422\n",
      "218 Rec loss:  229.03280639648438 KL_div:  13.477800369262695\n",
      "219 Rec loss:  228.6280059814453 KL_div:  13.660934448242188\n",
      "220 Rec loss:  228.24609375 KL_div:  14.184481620788574\n",
      "221 Rec loss:  228.69154357910156 KL_div:  13.593945503234863\n",
      "222 Rec loss:  228.2353057861328 KL_div:  13.963034629821777\n",
      "223 Rec loss:  228.1990966796875 KL_div:  13.935823440551758\n",
      "224 Rec loss:  228.38125610351562 KL_div:  13.682580947875977\n",
      "225 Rec loss:  228.55714416503906 KL_div:  13.59009075164795\n",
      "226 Rec loss:  228.1876220703125 KL_div:  13.892171859741211\n",
      "227 Rec loss:  228.06732177734375 KL_div:  14.010931015014648\n",
      "228 Rec loss:  228.0542755126953 KL_div:  13.881980895996094\n",
      "229 Rec loss:  228.04483032226562 KL_div:  13.81079387664795\n",
      "230 Rec loss:  228.11572265625 KL_div:  14.008404731750488\n",
      "231 Rec loss:  227.94110107421875 KL_div:  14.366903305053711\n",
      "232 Rec loss:  228.16415405273438 KL_div:  13.773560523986816\n",
      "233 Rec loss:  227.950927734375 KL_div:  13.832259178161621\n",
      "234 Rec loss:  227.7757568359375 KL_div:  14.101224899291992\n",
      "235 Rec loss:  227.82797241210938 KL_div:  13.9478120803833\n",
      "236 Rec loss:  228.44659423828125 KL_div:  13.524277687072754\n",
      "237 Rec loss:  228.1746063232422 KL_div:  13.623662948608398\n",
      "238 Rec loss:  228.0979766845703 KL_div:  13.789021492004395\n",
      "239 Rec loss:  227.906494140625 KL_div:  13.803262710571289\n",
      "240 Rec loss:  227.8975067138672 KL_div:  13.824590682983398\n",
      "241 Rec loss:  227.60922241210938 KL_div:  14.030241012573242\n",
      "242 Rec loss:  227.60595703125 KL_div:  14.356771469116211\n",
      "243 Rec loss:  227.8571319580078 KL_div:  13.911705017089844\n",
      "244 Rec loss:  227.97142028808594 KL_div:  13.755403518676758\n",
      "245 Rec loss:  227.59190368652344 KL_div:  14.077248573303223\n",
      "246 Rec loss:  227.92176818847656 KL_div:  13.769078254699707\n",
      "247 Rec loss:  227.5072021484375 KL_div:  14.085721969604492\n",
      "248 Rec loss:  227.51632690429688 KL_div:  13.940671920776367\n",
      "249 Rec loss:  228.10385131835938 KL_div:  13.578118324279785\n",
      "250 Rec loss:  227.4019012451172 KL_div:  14.112323760986328\n",
      "251 Rec loss:  227.461181640625 KL_div:  13.915066719055176\n",
      "252 Rec loss:  227.69927978515625 KL_div:  13.785183906555176\n",
      "253 Rec loss:  227.4324493408203 KL_div:  13.940539360046387\n",
      "254 Rec loss:  227.3081512451172 KL_div:  14.250575065612793\n",
      "255 Rec loss:  227.36109924316406 KL_div:  13.979290962219238\n",
      "256 Rec loss:  227.43421936035156 KL_div:  13.915875434875488\n",
      "257 Rec loss:  227.56500244140625 KL_div:  13.78952693939209\n",
      "258 Rec loss:  227.2378692626953 KL_div:  14.242549896240234\n",
      "259 Rec loss:  227.36524963378906 KL_div:  14.369026184082031\n",
      "260 Rec loss:  227.76304626464844 KL_div:  13.611775398254395\n",
      "261 Rec loss:  227.41075134277344 KL_div:  13.816225051879883\n",
      "262 Rec loss:  227.19886779785156 KL_div:  13.982637405395508\n",
      "263 Rec loss:  227.33799743652344 KL_div:  13.894883155822754\n",
      "264 Rec loss:  227.341552734375 KL_div:  13.857368469238281\n",
      "265 Rec loss:  227.1529541015625 KL_div:  14.124006271362305\n",
      "266 Rec loss:  227.140380859375 KL_div:  14.482196807861328\n",
      "267 Rec loss:  227.06982421875 KL_div:  13.997714042663574\n",
      "268 Rec loss:  227.76754760742188 KL_div:  13.62357234954834\n",
      "269 Rec loss:  227.01934814453125 KL_div:  14.322965621948242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270 Rec loss:  227.1885986328125 KL_div:  13.926846504211426\n",
      "271 Rec loss:  227.46812438964844 KL_div:  13.702373504638672\n",
      "272 Rec loss:  227.0191192626953 KL_div:  13.98148250579834\n",
      "273 Rec loss:  227.05838012695312 KL_div:  13.970406532287598\n",
      "274 Rec loss:  227.02835083007812 KL_div:  13.961189270019531\n",
      "275 Rec loss:  226.9850311279297 KL_div:  14.222722053527832\n",
      "276 Rec loss:  226.7987823486328 KL_div:  14.319843292236328\n",
      "277 Rec loss:  227.58932495117188 KL_div:  13.724888801574707\n",
      "278 Rec loss:  227.0367431640625 KL_div:  13.910018920898438\n",
      "279 Rec loss:  226.8970489501953 KL_div:  14.175945281982422\n",
      "280 Rec loss:  227.09805297851562 KL_div:  13.862153053283691\n",
      "281 Rec loss:  226.8332061767578 KL_div:  14.002012252807617\n",
      "282 Rec loss:  226.81202697753906 KL_div:  14.359034538269043\n",
      "283 Rec loss:  227.41409301757812 KL_div:  13.795271873474121\n",
      "284 Rec loss:  226.8406219482422 KL_div:  13.930062294006348\n",
      "285 Rec loss:  226.73841857910156 KL_div:  13.999682426452637\n",
      "286 Rec loss:  226.77279663085938 KL_div:  14.035265922546387\n",
      "287 Rec loss:  226.59120178222656 KL_div:  14.223103523254395\n",
      "288 Rec loss:  226.68299865722656 KL_div:  14.0574312210083\n",
      "289 Rec loss:  226.6879425048828 KL_div:  14.057421684265137\n",
      "290 Rec loss:  226.96957397460938 KL_div:  13.873673439025879\n",
      "291 Rec loss:  227.4697265625 KL_div:  13.660738945007324\n",
      "292 Rec loss:  226.56187438964844 KL_div:  14.197373390197754\n",
      "293 Rec loss:  226.51312255859375 KL_div:  14.215412139892578\n",
      "294 Rec loss:  226.91368103027344 KL_div:  13.883519172668457\n",
      "295 Rec loss:  226.77972412109375 KL_div:  13.933319091796875\n",
      "296 Rec loss:  226.4927520751953 KL_div:  14.249978065490723\n",
      "297 Rec loss:  226.3805694580078 KL_div:  14.261872291564941\n",
      "298 Rec loss:  226.40774536132812 KL_div:  14.378145217895508\n",
      "299 Rec loss:  226.571044921875 KL_div:  14.167322158813477\n",
      "300 Rec loss:  226.9314727783203 KL_div:  13.781545639038086\n",
      "301 Rec loss:  226.68014526367188 KL_div:  13.848968505859375\n",
      "302 Rec loss:  226.51829528808594 KL_div:  14.272893905639648\n",
      "303 Rec loss:  226.45037841796875 KL_div:  14.039382934570312\n",
      "304 Rec loss:  226.44471740722656 KL_div:  14.080609321594238\n",
      "305 Rec loss:  226.38462829589844 KL_div:  14.095969200134277\n",
      "306 Rec loss:  226.57460021972656 KL_div:  13.907647132873535\n",
      "307 Rec loss:  226.94529724121094 KL_div:  13.696943283081055\n",
      "308 Rec loss:  226.35067749023438 KL_div:  14.20081901550293\n",
      "309 Rec loss:  226.6655731201172 KL_div:  14.785120010375977\n",
      "310 Rec loss:  227.0647430419922 KL_div:  13.656365394592285\n",
      "311 Rec loss:  226.3034210205078 KL_div:  14.264531135559082\n",
      "312 Rec loss:  226.49227905273438 KL_div:  14.020395278930664\n",
      "313 Rec loss:  226.2711181640625 KL_div:  14.173253059387207\n",
      "314 Rec loss:  226.32437133789062 KL_div:  14.058950424194336\n",
      "315 Rec loss:  226.30203247070312 KL_div:  14.050088882446289\n",
      "316 Rec loss:  226.1622772216797 KL_div:  14.200270652770996\n",
      "317 Rec loss:  226.16819763183594 KL_div:  14.181160926818848\n",
      "318 Rec loss:  226.14736938476562 KL_div:  14.242334365844727\n",
      "319 Rec loss:  226.10272216796875 KL_div:  14.471532821655273\n",
      "320 Rec loss:  226.32275390625 KL_div:  13.960724830627441\n",
      "321 Rec loss:  226.5115966796875 KL_div:  13.880859375\n",
      "322 Rec loss:  225.9237060546875 KL_div:  14.333678245544434\n",
      "323 Rec loss:  226.02005004882812 KL_div:  14.221168518066406\n",
      "324 Rec loss:  226.55992126464844 KL_div:  13.904671669006348\n",
      "325 Rec loss:  226.17860412597656 KL_div:  13.98989486694336\n",
      "326 Rec loss:  225.9388427734375 KL_div:  14.513077735900879\n",
      "327 Rec loss:  226.25634765625 KL_div:  13.943915367126465\n",
      "328 Rec loss:  226.18222045898438 KL_div:  14.01016902923584\n",
      "329 Rec loss:  225.9490966796875 KL_div:  14.276851654052734\n",
      "330 Rec loss:  226.01809692382812 KL_div:  14.16220760345459\n",
      "331 Rec loss:  226.27976989746094 KL_div:  13.956903457641602\n",
      "332 Rec loss:  226.0148468017578 KL_div:  14.107048034667969\n",
      "333 Rec loss:  225.99639892578125 KL_div:  14.723820686340332\n",
      "334 Rec loss:  225.97584533691406 KL_div:  14.12308406829834\n",
      "335 Rec loss:  226.41976928710938 KL_div:  13.823531150817871\n",
      "336 Rec loss:  225.8460235595703 KL_div:  14.398490905761719\n",
      "337 Rec loss:  226.1000518798828 KL_div:  13.986759185791016\n",
      "338 Rec loss:  225.92715454101562 KL_div:  14.141378402709961\n",
      "339 Rec loss:  225.97909545898438 KL_div:  14.109381675720215\n",
      "340 Rec loss:  226.16819763183594 KL_div:  13.99879264831543\n",
      "341 Rec loss:  225.86093139648438 KL_div:  14.170804977416992\n",
      "342 Rec loss:  225.7969512939453 KL_div:  14.28449535369873\n",
      "343 Rec loss:  225.69834899902344 KL_div:  14.401803016662598\n",
      "344 Rec loss:  225.7808074951172 KL_div:  14.333237648010254\n",
      "345 Rec loss:  226.70108032226562 KL_div:  13.830220222473145\n",
      "346 Rec loss:  226.01718139648438 KL_div:  13.964715957641602\n",
      "347 Rec loss:  225.740478515625 KL_div:  14.217785835266113\n",
      "348 Rec loss:  225.81704711914062 KL_div:  14.097856521606445\n",
      "349 Rec loss:  225.68914794921875 KL_div:  14.2023344039917\n",
      "350 Rec loss:  225.77049255371094 KL_div:  14.056865692138672\n",
      "351 Rec loss:  225.78717041015625 KL_div:  14.09570598602295\n",
      "352 Rec loss:  225.74215698242188 KL_div:  14.087322235107422\n",
      "353 Rec loss:  225.53379821777344 KL_div:  14.46843433380127\n",
      "354 Rec loss:  225.9384765625 KL_div:  13.936262130737305\n",
      "355 Rec loss:  225.64724731445312 KL_div:  14.267971992492676\n",
      "356 Rec loss:  225.79640197753906 KL_div:  14.056893348693848\n",
      "357 Rec loss:  225.9290008544922 KL_div:  13.944896697998047\n",
      "358 Rec loss:  225.51107788085938 KL_div:  14.28112506866455\n",
      "359 Rec loss:  225.5697479248047 KL_div:  14.166025161743164\n",
      "360 Rec loss:  225.60906982421875 KL_div:  14.263626098632812\n",
      "361 Rec loss:  225.52517700195312 KL_div:  14.276987075805664\n",
      "362 Rec loss:  225.5980224609375 KL_div:  14.366190910339355\n",
      "363 Rec loss:  225.61563110351562 KL_div:  14.682809829711914\n",
      "364 Rec loss:  225.91134643554688 KL_div:  13.932463645935059\n",
      "365 Rec loss:  225.52655029296875 KL_div:  14.251782417297363\n",
      "366 Rec loss:  225.44447326660156 KL_div:  14.236746788024902\n",
      "367 Rec loss:  225.68067932128906 KL_div:  14.062468528747559\n",
      "368 Rec loss:  225.28485107421875 KL_div:  14.408942222595215\n",
      "369 Rec loss:  225.43722534179688 KL_div:  14.18768310546875\n",
      "370 Rec loss:  225.68609619140625 KL_div:  14.017325401306152\n",
      "371 Rec loss:  225.5696258544922 KL_div:  14.091634750366211\n",
      "372 Rec loss:  225.56883239746094 KL_div:  14.122418403625488\n",
      "373 Rec loss:  225.25164794921875 KL_div:  14.346285820007324\n",
      "374 Rec loss:  225.468017578125 KL_div:  14.127376556396484\n",
      "375 Rec loss:  225.7031707763672 KL_div:  14.008384704589844\n",
      "376 Rec loss:  226.2969207763672 KL_div:  13.745526313781738\n",
      "377 Rec loss:  225.212646484375 KL_div:  14.422195434570312\n",
      "378 Rec loss:  225.3872528076172 KL_div:  14.249834060668945\n",
      "379 Rec loss:  225.63278198242188 KL_div:  13.970069885253906\n",
      "380 Rec loss:  225.47535705566406 KL_div:  14.116668701171875\n",
      "381 Rec loss:  225.13272094726562 KL_div:  14.338345527648926\n",
      "382 Rec loss:  225.30345153808594 KL_div:  14.207378387451172\n",
      "383 Rec loss:  225.45034790039062 KL_div:  14.08908462524414\n",
      "384 Rec loss:  225.20852661132812 KL_div:  14.295609474182129\n",
      "385 Rec loss:  225.17979431152344 KL_div:  14.356521606445312\n",
      "386 Rec loss:  225.25070190429688 KL_div:  14.20580768585205\n",
      "387 Rec loss:  225.09429931640625 KL_div:  14.417501449584961\n",
      "388 Rec loss:  225.1410675048828 KL_div:  14.413174629211426\n",
      "389 Rec loss:  225.20558166503906 KL_div:  14.244462966918945\n",
      "390 Rec loss:  225.4465789794922 KL_div:  14.052387237548828\n",
      "391 Rec loss:  225.57647705078125 KL_div:  13.94993782043457\n",
      "392 Rec loss:  225.79220581054688 KL_div:  13.975959777832031\n",
      "393 Rec loss:  225.24420166015625 KL_div:  14.187707901000977\n",
      "394 Rec loss:  225.14308166503906 KL_div:  14.24605941772461\n",
      "395 Rec loss:  225.14805603027344 KL_div:  14.13845157623291\n",
      "396 Rec loss:  225.28372192382812 KL_div:  14.139046669006348\n",
      "397 Rec loss:  225.70315551757812 KL_div:  13.88992691040039\n",
      "398 Rec loss:  225.1644744873047 KL_div:  14.186339378356934\n",
      "399 Rec loss:  225.00987243652344 KL_div:  14.40256118774414\n",
      "400 Rec loss:  225.13699340820312 KL_div:  14.62954330444336\n",
      "401 Rec loss:  225.1486053466797 KL_div:  14.187047004699707\n",
      "402 Rec loss:  225.17837524414062 KL_div:  14.141434669494629\n",
      "403 Rec loss:  225.1045684814453 KL_div:  14.242077827453613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 Rec loss:  225.04977416992188 KL_div:  14.284957885742188\n",
      "405 Rec loss:  225.16685485839844 KL_div:  14.087176322937012\n",
      "406 Rec loss:  224.98497009277344 KL_div:  14.618640899658203\n",
      "407 Rec loss:  225.24795532226562 KL_div:  14.033724784851074\n",
      "408 Rec loss:  225.606201171875 KL_div:  13.877848625183105\n",
      "409 Rec loss:  224.99154663085938 KL_div:  14.250640869140625\n",
      "410 Rec loss:  225.06031799316406 KL_div:  14.250078201293945\n",
      "411 Rec loss:  225.06785583496094 KL_div:  14.222740173339844\n",
      "412 Rec loss:  225.0799560546875 KL_div:  14.247734069824219\n",
      "413 Rec loss:  225.07864379882812 KL_div:  14.129072189331055\n",
      "414 Rec loss:  224.83457946777344 KL_div:  14.336470603942871\n",
      "415 Rec loss:  224.91769409179688 KL_div:  14.22293758392334\n",
      "416 Rec loss:  224.92626953125 KL_div:  14.242818832397461\n",
      "417 Rec loss:  225.32005310058594 KL_div:  13.9295654296875\n",
      "418 Rec loss:  225.09542846679688 KL_div:  14.07717227935791\n",
      "419 Rec loss:  224.80494689941406 KL_div:  14.32125473022461\n",
      "420 Rec loss:  224.80831909179688 KL_div:  14.334321975708008\n",
      "421 Rec loss:  225.0611572265625 KL_div:  14.103009223937988\n",
      "422 Rec loss:  225.1647186279297 KL_div:  14.013837814331055\n",
      "423 Rec loss:  225.05650329589844 KL_div:  14.142147064208984\n",
      "424 Rec loss:  224.84872436523438 KL_div:  14.220548629760742\n",
      "425 Rec loss:  224.71009826660156 KL_div:  14.418228149414062\n",
      "426 Rec loss:  224.75537109375 KL_div:  14.743300437927246\n",
      "427 Rec loss:  224.9541473388672 KL_div:  14.17755126953125\n",
      "428 Rec loss:  224.97999572753906 KL_div:  14.104951858520508\n",
      "429 Rec loss:  224.95260620117188 KL_div:  14.137640953063965\n",
      "430 Rec loss:  224.79269409179688 KL_div:  14.27497386932373\n",
      "431 Rec loss:  224.72845458984375 KL_div:  14.405699729919434\n",
      "432 Rec loss:  224.89324951171875 KL_div:  14.224662780761719\n",
      "433 Rec loss:  224.67591857910156 KL_div:  14.438101768493652\n",
      "434 Rec loss:  224.8759002685547 KL_div:  14.587312698364258\n",
      "435 Rec loss:  224.74627685546875 KL_div:  14.498895645141602\n",
      "436 Rec loss:  225.2967071533203 KL_div:  13.973544120788574\n",
      "437 Rec loss:  224.63075256347656 KL_div:  14.586625099182129\n",
      "438 Rec loss:  224.92320251464844 KL_div:  14.1358003616333\n",
      "439 Rec loss:  224.61936950683594 KL_div:  14.364640235900879\n",
      "440 Rec loss:  224.77122497558594 KL_div:  14.199118614196777\n",
      "441 Rec loss:  224.73336791992188 KL_div:  14.216490745544434\n",
      "442 Rec loss:  224.76487731933594 KL_div:  14.154243469238281\n",
      "443 Rec loss:  224.529052734375 KL_div:  14.390546798706055\n",
      "444 Rec loss:  224.85096740722656 KL_div:  14.063467025756836\n",
      "445 Rec loss:  224.51510620117188 KL_div:  14.458477973937988\n",
      "446 Rec loss:  224.67315673828125 KL_div:  14.206348419189453\n",
      "447 Rec loss:  224.74020385742188 KL_div:  14.171131134033203\n",
      "448 Rec loss:  224.59217834472656 KL_div:  14.227739334106445\n",
      "449 Rec loss:  224.735107421875 KL_div:  14.09938907623291\n",
      "450 Rec loss:  224.73965454101562 KL_div:  14.151176452636719\n",
      "451 Rec loss:  224.51502990722656 KL_div:  14.551146507263184\n",
      "452 Rec loss:  224.71961975097656 KL_div:  14.21125316619873\n",
      "453 Rec loss:  225.21852111816406 KL_div:  13.926750183105469\n",
      "454 Rec loss:  224.52630615234375 KL_div:  14.32441234588623\n",
      "455 Rec loss:  224.4115447998047 KL_div:  14.340230941772461\n",
      "456 Rec loss:  224.8231964111328 KL_div:  14.088027000427246\n",
      "457 Rec loss:  224.5656280517578 KL_div:  14.255775451660156\n",
      "458 Rec loss:  224.5508575439453 KL_div:  14.321027755737305\n",
      "459 Rec loss:  224.5041046142578 KL_div:  14.346405029296875\n",
      "460 Rec loss:  224.82864379882812 KL_div:  14.03869342803955\n",
      "461 Rec loss:  225.14556884765625 KL_div:  13.927773475646973\n",
      "462 Rec loss:  224.5591278076172 KL_div:  14.155924797058105\n",
      "463 Rec loss:  224.3407440185547 KL_div:  14.485462188720703\n",
      "464 Rec loss:  224.43519592285156 KL_div:  14.387022972106934\n",
      "465 Rec loss:  224.34475708007812 KL_div:  14.379297256469727\n",
      "466 Rec loss:  224.7176055908203 KL_div:  14.079676628112793\n",
      "467 Rec loss:  224.58888244628906 KL_div:  14.11727523803711\n",
      "468 Rec loss:  224.35617065429688 KL_div:  14.481049537658691\n",
      "469 Rec loss:  224.32974243164062 KL_div:  14.341535568237305\n",
      "470 Rec loss:  224.6085968017578 KL_div:  14.156046867370605\n",
      "471 Rec loss:  224.4035186767578 KL_div:  14.25806999206543\n",
      "472 Rec loss:  224.39100646972656 KL_div:  14.241009712219238\n",
      "473 Rec loss:  224.2964324951172 KL_div:  14.302800178527832\n",
      "474 Rec loss:  224.14657592773438 KL_div:  14.570417404174805\n",
      "475 Rec loss:  224.75247192382812 KL_div:  14.462095260620117\n",
      "476 Rec loss:  224.3851776123047 KL_div:  14.47690486907959\n",
      "477 Rec loss:  224.6216278076172 KL_div:  14.16425609588623\n",
      "478 Rec loss:  224.4554443359375 KL_div:  14.323447227478027\n",
      "479 Rec loss:  224.38980102539062 KL_div:  14.210230827331543\n",
      "480 Rec loss:  224.3842010498047 KL_div:  14.251893997192383\n",
      "481 Rec loss:  224.73809814453125 KL_div:  14.036727905273438\n",
      "482 Rec loss:  224.55075073242188 KL_div:  14.049806594848633\n",
      "483 Rec loss:  224.14756774902344 KL_div:  14.524234771728516\n",
      "484 Rec loss:  224.2415771484375 KL_div:  14.314090728759766\n",
      "485 Rec loss:  224.20257568359375 KL_div:  14.301473617553711\n",
      "486 Rec loss:  224.33624267578125 KL_div:  14.254790306091309\n",
      "487 Rec loss:  224.24395751953125 KL_div:  14.260315895080566\n",
      "488 Rec loss:  224.67959594726562 KL_div:  13.970179557800293\n",
      "489 Rec loss:  225.3165740966797 KL_div:  13.759871482849121\n",
      "490 Rec loss:  224.0270233154297 KL_div:  14.620573043823242\n",
      "491 Rec loss:  224.33277893066406 KL_div:  14.24634075164795\n",
      "492 Rec loss:  224.21957397460938 KL_div:  14.25410270690918\n",
      "493 Rec loss:  224.3372039794922 KL_div:  14.155858993530273\n",
      "494 Rec loss:  224.19317626953125 KL_div:  14.471755981445312\n",
      "495 Rec loss:  225.2495574951172 KL_div:  13.655596733093262\n",
      "496 Rec loss:  224.0780487060547 KL_div:  14.421934127807617\n",
      "497 Rec loss:  224.20469665527344 KL_div:  14.416576385498047\n",
      "498 Rec loss:  224.20254516601562 KL_div:  14.295531272888184\n",
      "499 Rec loss:  224.1613311767578 KL_div:  14.345893859863281\n",
      "500 Rec loss:  224.27565002441406 KL_div:  14.191896438598633\n",
      "501 Rec loss:  224.1892547607422 KL_div:  14.301846504211426\n",
      "502 Rec loss:  224.35910034179688 KL_div:  14.080927848815918\n",
      "503 Rec loss:  224.3553466796875 KL_div:  14.086284637451172\n",
      "504 Rec loss:  224.08152770996094 KL_div:  14.259770393371582\n",
      "505 Rec loss:  224.18267822265625 KL_div:  14.255125045776367\n",
      "506 Rec loss:  224.07192993164062 KL_div:  14.338231086730957\n",
      "507 Rec loss:  224.12973022460938 KL_div:  14.306757926940918\n",
      "508 Rec loss:  224.2366180419922 KL_div:  14.285858154296875\n",
      "509 Rec loss:  223.9553680419922 KL_div:  14.590287208557129\n",
      "510 Rec loss:  224.37374877929688 KL_div:  14.060934066772461\n",
      "511 Rec loss:  224.25755310058594 KL_div:  14.124735832214355\n",
      "512 Rec loss:  224.1991729736328 KL_div:  14.26175594329834\n",
      "513 Rec loss:  224.14100646972656 KL_div:  14.28827953338623\n",
      "514 Rec loss:  224.12925720214844 KL_div:  14.20683479309082\n",
      "515 Rec loss:  224.5146484375 KL_div:  13.964072227478027\n",
      "516 Rec loss:  224.59007263183594 KL_div:  14.014223098754883\n",
      "517 Rec loss:  224.18997192382812 KL_div:  14.143596649169922\n",
      "518 Rec loss:  223.93319702148438 KL_div:  14.606244087219238\n",
      "519 Rec loss:  223.9771270751953 KL_div:  14.398143768310547\n",
      "520 Rec loss:  223.9107208251953 KL_div:  14.389281272888184\n",
      "521 Rec loss:  223.9281005859375 KL_div:  14.314884185791016\n",
      "522 Rec loss:  224.16036987304688 KL_div:  14.088973045349121\n",
      "523 Rec loss:  224.05580139160156 KL_div:  14.265199661254883\n",
      "524 Rec loss:  223.99205017089844 KL_div:  14.740677833557129\n",
      "525 Rec loss:  224.2165985107422 KL_div:  14.217961311340332\n",
      "526 Rec loss:  224.3751220703125 KL_div:  14.144315719604492\n",
      "527 Rec loss:  223.96424865722656 KL_div:  14.401334762573242\n",
      "528 Rec loss:  224.33975219726562 KL_div:  13.996603012084961\n",
      "529 Rec loss:  224.12387084960938 KL_div:  14.177190780639648\n",
      "530 Rec loss:  223.86756896972656 KL_div:  14.380215644836426\n",
      "531 Rec loss:  224.0416259765625 KL_div:  14.192837715148926\n",
      "532 Rec loss:  223.80520629882812 KL_div:  14.516080856323242\n",
      "533 Rec loss:  224.0376434326172 KL_div:  14.235818862915039\n",
      "534 Rec loss:  224.0758056640625 KL_div:  14.208076477050781\n",
      "535 Rec loss:  223.85069274902344 KL_div:  14.40024185180664\n",
      "536 Rec loss:  223.78932189941406 KL_div:  14.437870025634766\n",
      "537 Rec loss:  223.85647583007812 KL_div:  14.50151538848877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538 Rec loss:  223.9896697998047 KL_div:  14.195223808288574\n",
      "539 Rec loss:  224.2412567138672 KL_div:  14.114896774291992\n",
      "540 Rec loss:  223.7060546875 KL_div:  14.635849952697754\n",
      "541 Rec loss:  223.92889404296875 KL_div:  14.327012062072754\n",
      "542 Rec loss:  224.17849731445312 KL_div:  14.176887512207031\n",
      "543 Rec loss:  224.36849975585938 KL_div:  13.96109676361084\n",
      "544 Rec loss:  223.71629333496094 KL_div:  14.533878326416016\n",
      "545 Rec loss:  223.95594787597656 KL_div:  14.164576530456543\n",
      "546 Rec loss:  223.8144989013672 KL_div:  14.366606712341309\n",
      "547 Rec loss:  223.6996307373047 KL_div:  14.5260591506958\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5d37d11c3824>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_db\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# x: [b, 28, 28] => [-1, 28*28]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/tf2_py3/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/tf2_py3/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \"\"\"\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/tf2_py3/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/tf2_py3/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m         \u001b[0;34m\"IteratorGetNextSync\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m         \"output_types\", output_types, \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   1941\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    for step, x in enumerate(train_db):\n",
    "        \n",
    "        # x: [b, 28, 28] => [-1, 28*28]\n",
    "        x = tf.reshape(x, [-1, 28*28])\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            x_rec_logits, h_mean, h_var = VAE_model(x)\n",
    "#             print(x_rec_logits)\n",
    "            # 经过VAE 重构的 x_rec_logits 与 x 之间的损失 rec_loss \n",
    "            rec_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=x_rec_logits)\n",
    "            # [b, 10] => scalar标量   tf.reduce_sum()  求出所有行和列的和\n",
    "            rec_loss = tf.reduce_sum(rec_loss) / x.shape[0]\n",
    "#             print(x.shape[0])\n",
    "\n",
    "            # https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians\n",
    "            # compute kl divergence散度  p ~  N(h_mean, h_var)  与 q ~  N(0, 1)  的散度\n",
    "            # 潜在分布Z ~ N( h_mean, h_val)  与 标准正态分布 N(0, 1)  的KL散度\n",
    "            # h_mean: [b, 10], h_var: [b, 10]\n",
    "#             print(h_mean)\n",
    "#             print(h_var)\n",
    "#             kl_div = -0.5 * (1 - h_val - h_mean**2 + tf.math.log(h_var))  \n",
    "            kl_div = -0.5 * (h_var + 1 - h_mean**2 - tf.exp(h_var))\n",
    "#             print(kl_div.shape)    [512, 10]\n",
    "#             print(tf.reduce_sum(kl_div))\n",
    "    \n",
    "            kl_div = tf.reduce_sum(kl_div) / x.shape[0]\n",
    "#             print(kl_div)\n",
    "            # 其中 1. 属于超参数，用于权衡rec_loss 与 kl_div\n",
    "            loss = rec_loss + 1. * kl_div\n",
    "            \n",
    "        grads = tape.gradient(loss, VAE_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, VAE_model.trainable_variables))\n",
    "        \n",
    "    if epoch % 1 == 0:\n",
    "            print(epoch, 'Rec loss: ', float(rec_loss), 'KL_div: ', float(kl_div))\n",
    "            \n",
    "    # evalution 测试评估\n",
    "    # 对于随机生成的数据\n",
    "    if epoch % 10 == 0:\n",
    "        z = tf.random.normal((batchsz, z_dim))\n",
    "        logits = VAE_model.decoder(z)\n",
    "        x_hat = tf.sigmoid(logits)\n",
    "        x_hat = tf.reshape(x_hat, [-1, 28, 28]).numpy() * 255.\n",
    "        x_hat = x_hat.astype(np.uint8)\n",
    "        save_image(x_hat, '/home/kukafee/workspace/picture/pic1/sampled_epoch_%d.png'%epoch)\n",
    "    \n",
    "    # 对于测试集中数据\n",
    "    if epoch % 10 == 0:\n",
    "        x_test = next(iter(test_db))\n",
    "        x_test = tf.reshape(x_test, [-1, 28*28])\n",
    "        x_hat_logits, _, _ = VAE_model(x_test)\n",
    "        x_hat = tf.sigmoid(x_hat_logits)\n",
    "        x_hat = tf.reshape(x_hat, [-1, 28, 28]).numpy() * 255.\n",
    "        x_hat = x_hat.astype(np.uint8)\n",
    "        save_image(x_hat, '/home/kukafee/workspace/picture/pic2/rec_epoch_%d.png'%epoch)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

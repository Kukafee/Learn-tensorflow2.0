{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet18 or 34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结：对于残差网络ResNet18层，学习率设置为0.001，对CIFAR100经过近100epoch的训练，由于梯度离散和过拟合的发生，loss值很小，准确率也没有明显提升；然后把学习率设置为0.01，结果出出现梯度爆炸现象；把训练网络换成了RetNet34，学习率设置为0.0001，学习效率很快。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "etNet34，学习率设置为(1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential, datasets, optimizers\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "tf.random.set_seed(2345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 BasicBlock类（残差基本单元）\n",
    "class BasicBlock(layers.Layer):\n",
    "    # 其中 filter_num 是该卷积核通道的数量，是把输入进该卷积核的channel转换为数量为filter_num大小\n",
    "    def __init__(self, filter_num, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        # 定义BasicBlock中的第一个卷积单元核，包括3*3的卷积、BachNom、ReLU。\n",
    "        # 这个卷积核接受一个输入数据 [b, h, w, c],\n",
    "        self.conv1 = layers.Conv2D(filter_num, kernel_size=[3, 3], strides=stride, padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.Activation('relu')\n",
    "\n",
    "        # 定义BasicBlock中的第二个卷积单元变量\n",
    "        # 其中strides 设置为1，表示为不间隔进行卷积操作，不减少featureMap\n",
    "        self.conv2 = layers.Conv2D(filter_num, kernel_size=[3, 3], strides=1, padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.relu2 = layers.Activation('relu')\n",
    "\n",
    "        # 定义残差网络中，短接的那一部分；分为stride为1和不为1两种情况，满足最后加和条件\n",
    "        if stride != 1:\n",
    "            self.downsample = Sequential()\n",
    "            self.downsample.add(layers.Conv2D(filter_num, kernel_size=[1, 1], strides=stride))\n",
    "        else:\n",
    "            self.downsample = lambda x : x\n",
    "            \n",
    "    # call() 在调用该类时调用这个call() 方法，实现前向传播，在调用BasicBlock类对象时，执行该方法，\n",
    "    # 第一个参数是输入该神经网络的数据 [b, h, w, c]\n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs [b, h, w, c]\n",
    "        out1 = self.conv1(inputs)\n",
    "        out1 = self.bn1(out1)\n",
    "        out1 = self.relu1(out1)\n",
    "        \n",
    "        out2 = self.conv2(out1)\n",
    "        out2 = self.bn2(out2)\n",
    "        \n",
    "        identity_out = self.downsample(inputs)\n",
    "        output = layers.add([out2, identity_out])    # 调用layers.add() 实现对应元素相加\n",
    "        output = self.relu2(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "# 定义RetNet 类（包括定义残差块build_RestBlock，包括两个基本的残差基本单元）\n",
    "class RetNet(keras.Model):\n",
    "    # 定义初始化函数\n",
    "    def __init__(self, layer_dims, num_classes=100):\n",
    "        super(RetNet, self).__init__()\n",
    "        # 定义预处理的一个卷积层\n",
    "        self.stem = Sequential([\n",
    "            layers.Conv2D(64, kernel_size=[3, 3], strides=1),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation('relu'),\n",
    "            layers.MaxPool2D(pool_size=(2, 2), strides=1, padding='same')\n",
    "        ])\n",
    "        # 创建4个RestBlock，指定每个的channel（filter数量），\n",
    "        # 其中从第二个起设置stride=2，以达到减小featureMap的目的\n",
    "        self.layer1 = self.build_RestBlock(64, layer_dims[0])\n",
    "        self.layer2 = self.build_RestBlock(128, layer_dims[1], stride=2)\n",
    "        self.layer3 = self.build_RestBlock(256, layer_dims[2], stride=2)\n",
    "        self.layer4 = self.build_RestBlock(512, layer_dims[3], stride=2)\n",
    "        # 考虑到全连接层输入决定于前面层的输出 [b, h, w, 512]，需要降维打平操作\n",
    "        # 调用layers.GlobalAveragePooling2D()方法，简称GAP，全局池化层\n",
    "        # 具体可参考：https://www.cnblogs.com/hutao722/p/10008581.html\n",
    "        self.avgpool = layers.GlobalAveragePooling2D()\n",
    "        # 创建全连接层，用来分类,参数是输出类别数\n",
    "        self.fc = layers.Dense(num_classes)\n",
    "    # 前向传播    \n",
    "    def call(self, inputs, training=None):\n",
    "        # 预处理卷积层\n",
    "        x = self.stem(inputs)\n",
    "        # 四个残差块，包括8个残差单元，一个经过16个卷积核\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        # 全池化层 [b, c]\n",
    "        x = self.avgpool(x)\n",
    "        # 全连接层 [b, 100]\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # 定义残差块，包括两个残差基本单元，filter_num该残差块中通道的数量，blocks表示包含的残差基本单元数\n",
    "    def build_RestBlock(self, filter_num, blocks, stride=1):\n",
    "        # 定义神经网络容器Sequential\n",
    "        res_blocks = Sequential()\n",
    "        # 在res_blocks添加层（残差基本单元BasicBlock），可能发生下采样\n",
    "        res_blocks.add(BasicBlock(filter_num, stride))\n",
    "        # \n",
    "        for _ in range(1, blocks):    # 设置为从1到 blocks，其中包括1，但不包括 blocks\n",
    "            # 不允许下采样，因为在残差单元中要保存featureMap不变，便于与identity进行加和操作\n",
    "            res_blocks.add(BasicBlock(filter_num, stride=1))\n",
    "            \n",
    "        return res_blocks\n",
    "    \n",
    "# 定义ResNet18 [2, 2, 2, 2]\n",
    "def resnet18():\n",
    "    return RetNet([2, 2, 2, 2])\n",
    "    \n",
    "# 定义ResNet34 [3, 4, 6, 3]  \n",
    "def resnet34():\n",
    "    return RetNet([3, 4, 6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_train:  (512, 32, 32, 3) (512,) tf.Tensor(-0.5, shape=(), dtype=float32) tf.Tensor(99, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 定义预处理函数\n",
    "def preprocess(x, y):\n",
    "    # [-0.5 ~ 0.5]\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255. - 0.5\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x, y\n",
    "\n",
    "# 加载数据集和数据集预处理\n",
    "(x, y), (x_test, y_test) = datasets.cifar100.load_data()\n",
    "y = tf.squeeze(y, axis=1)\n",
    "y_test = tf.squeeze(y_test, axis=1)\n",
    "\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "train_db = train_db.shuffle(10000).map(preprocess).batch(512)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_db = test_db.map(preprocess).batch(256)\n",
    "\n",
    "# 查看数据集切片\n",
    "sample = next(iter(train_db))\n",
    "print('sample_train: ', sample[0].shape, sample[1].shape, tf.reduce_min(sample[0]),\n",
    "     tf.reduce_max(sample[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ret_net\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      multiple                  2048      \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    multiple                  223104    \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    multiple                  1119360   \n",
      "_________________________________________________________________\n",
      "sequential_4 (Sequential)    multiple                  6831360   \n",
      "_________________________________________________________________\n",
      "sequential_6 (Sequential)    multiple                  13123072  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  51300     \n",
      "=================================================================\n",
      "Total params: 21,350,244\n",
      "Trainable params: 21,335,012\n",
      "Non-trainable params: 15,232\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 生成ResNet18网络\n",
    "# model = resnet18()\n",
    "# model.build(input_shape=(None, 32, 32, 3))\n",
    "# model.summary()\n",
    "\n",
    "# 生成ResNet34网络\n",
    "model = resnet34()\n",
    "model.build(input_shape=(None, 32, 32, 3))\n",
    "model.summary()\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optimizers.Adam(lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss:  4.612636089324951\n",
      "0 acc:  0.0513\n",
      "1 0 loss:  4.215523719787598\n",
      "1 acc:  0.0881\n",
      "2 0 loss:  3.9350132942199707\n",
      "2 acc:  0.1167\n",
      "3 0 loss:  3.7696704864501953\n",
      "3 acc:  0.1314\n",
      "4 0 loss:  3.6653172969818115\n",
      "4 acc:  0.1488\n",
      "5 0 loss:  3.5529751777648926\n",
      "5 acc:  0.1618\n",
      "6 0 loss:  3.4723737239837646\n",
      "6 acc:  0.1783\n",
      "7 0 loss:  3.384908676147461\n",
      "7 acc:  0.1922\n",
      "8 0 loss:  3.306673526763916\n",
      "8 acc:  0.2046\n",
      "9 0 loss:  3.2464451789855957\n",
      "9 acc:  0.2138\n",
      "10 0 loss:  3.1760494709014893\n",
      "10 acc:  0.2186\n",
      "11 0 loss:  3.1178526878356934\n",
      "11 acc:  0.225\n",
      "12 0 loss:  3.0586466789245605\n",
      "12 acc:  0.2359\n",
      "13 0 loss:  3.0066609382629395\n",
      "13 acc:  0.2437\n",
      "14 0 loss:  2.953023672103882\n",
      "14 acc:  0.2483\n",
      "15 0 loss:  2.9149017333984375\n",
      "15 acc:  0.2482\n",
      "16 0 loss:  2.8877527713775635\n",
      "16 acc:  0.2571\n",
      "17 0 loss:  2.8234970569610596\n",
      "17 acc:  0.2604\n",
      "18 0 loss:  2.784043550491333\n",
      "18 acc:  0.2658\n",
      "19 0 loss:  2.745532751083374\n",
      "19 acc:  0.271\n",
      "20 0 loss:  2.705897569656372\n",
      "20 acc:  0.2774\n",
      "21 0 loss:  2.6695454120635986\n",
      "21 acc:  0.2811\n",
      "22 0 loss:  2.6352450847625732\n",
      "22 acc:  0.2813\n",
      "23 0 loss:  2.615138292312622\n",
      "23 acc:  0.2863\n",
      "24 0 loss:  2.5907249450683594\n",
      "24 acc:  0.2905\n",
      "25 0 loss:  2.5753400325775146\n",
      "25 acc:  0.2819\n",
      "26 0 loss:  2.6044423580169678\n",
      "26 acc:  0.2828\n",
      "27 0 loss:  2.5691094398498535\n",
      "27 acc:  0.2857\n",
      "28 0 loss:  2.537771463394165\n",
      "28 acc:  0.2862\n",
      "29 0 loss:  2.514744520187378\n",
      "29 acc:  0.2883\n",
      "30 0 loss:  2.4857022762298584\n",
      "30 acc:  0.2896\n",
      "31 0 loss:  2.4576311111450195\n",
      "31 acc:  0.2865\n",
      "32 0 loss:  2.4550704956054688\n",
      "32 acc:  0.2928\n",
      "33 0 loss:  2.3974359035491943\n",
      "33 acc:  0.2975\n",
      "34 0 loss:  2.3475706577301025\n",
      "34 acc:  0.3016\n",
      "35 0 loss:  2.3029732704162598\n",
      "35 acc:  0.3067\n",
      "36 0 loss:  2.2616631984710693\n",
      "36 acc:  0.3118\n",
      "37 0 loss:  2.2283811569213867\n",
      "37 acc:  0.3136\n",
      "38 0 loss:  2.1985690593719482\n",
      "38 acc:  0.3148\n",
      "39 0 loss:  2.1698811054229736\n",
      "39 acc:  0.3163\n",
      "40 0 loss:  2.138096809387207\n",
      "40 acc:  0.3165\n",
      "41 0 loss:  2.1057281494140625\n",
      "41 acc:  0.3184\n",
      "42 0 loss:  2.0680713653564453\n",
      "42 acc:  0.3216\n",
      "43 0 loss:  2.0337953567504883\n",
      "43 acc:  0.3217\n",
      "44 0 loss:  2.0002031326293945\n",
      "44 acc:  0.32\n",
      "45 0 loss:  1.9756861925125122\n",
      "45 acc:  0.3051\n",
      "46 0 loss:  2.0516085624694824\n",
      "46 acc:  0.3105\n",
      "47 0 loss:  2.002337694168091\n",
      "47 acc:  0.3235\n",
      "48 0 loss:  1.945048451423645\n",
      "48 acc:  0.3254\n",
      "49 0 loss:  1.9548665285110474\n",
      "49 acc:  0.3219\n",
      "50 0 loss:  2.0081019401550293\n",
      "50 acc:  0.3214\n",
      "51 0 loss:  1.9679443836212158\n",
      "51 acc:  0.3203\n",
      "52 0 loss:  1.9328941106796265\n",
      "52 acc:  0.3201\n",
      "53 0 loss:  1.8986107110977173\n",
      "53 acc:  0.3207\n",
      "54 0 loss:  1.861163854598999\n",
      "54 acc:  0.3246\n",
      "55 0 loss:  1.797184705734253\n",
      "55 acc:  0.329\n",
      "56 0 loss:  1.7249879837036133\n",
      "56 acc:  0.3279\n",
      "57 0 loss:  1.7328754663467407\n",
      "57 acc:  0.3252\n",
      "58 0 loss:  1.7828861474990845\n",
      "58 acc:  0.3213\n",
      "59 0 loss:  1.777171015739441\n",
      "59 acc:  0.3187\n",
      "60 0 loss:  1.769928216934204\n",
      "60 acc:  0.3171\n",
      "61 0 loss:  1.7633486986160278\n",
      "61 acc:  0.3179\n",
      "62 0 loss:  1.7438591718673706\n",
      "62 acc:  0.3195\n",
      "63 0 loss:  1.6982965469360352\n",
      "63 acc:  0.3283\n",
      "64 0 loss:  1.6212379932403564\n",
      "64 acc:  0.33\n",
      "65 0 loss:  1.5760018825531006\n",
      "65 acc:  0.3262\n",
      "66 0 loss:  1.5635998249053955\n",
      "66 acc:  0.3233\n",
      "67 0 loss:  1.5645101070404053\n",
      "67 acc:  0.3197\n",
      "68 0 loss:  1.564283847808838\n",
      "68 acc:  0.3143\n",
      "69 0 loss:  1.5698097944259644\n",
      "69 acc:  0.3122\n",
      "70 0 loss:  1.5741674900054932\n",
      "70 acc:  0.3102\n",
      "71 0 loss:  1.5653585195541382\n",
      "71 acc:  0.3159\n",
      "72 0 loss:  1.497989535331726\n",
      "72 acc:  0.3203\n",
      "73 0 loss:  1.5085244178771973\n",
      "73 acc:  0.317\n",
      "74 0 loss:  1.5309371948242188\n",
      "74 acc:  0.3167\n",
      "75 0 loss:  1.4738247394561768\n",
      "75 acc:  0.3164\n",
      "76 0 loss:  1.416911244392395\n",
      "76 acc:  0.3166\n",
      "77 0 loss:  1.380363941192627\n",
      "77 acc:  0.3169\n",
      "78 0 loss:  1.3628735542297363\n",
      "78 acc:  0.3135\n",
      "79 0 loss:  1.371659278869629\n",
      "79 acc:  0.3079\n",
      "80 0 loss:  1.4251943826675415\n",
      "80 acc:  0.31\n",
      "81 0 loss:  1.4521880149841309\n",
      "81 acc:  0.3102\n",
      "82 0 loss:  1.387316107749939\n",
      "82 acc:  0.3116\n",
      "83 0 loss:  1.3538174629211426\n",
      "83 acc:  0.3118\n",
      "84 0 loss:  1.32142972946167\n",
      "84 acc:  0.3102\n",
      "85 0 loss:  1.3147369623184204\n",
      "85 acc:  0.3076\n",
      "86 0 loss:  1.3099123239517212\n",
      "86 acc:  0.3119\n",
      "87 0 loss:  1.2405061721801758\n",
      "87 acc:  0.3072\n",
      "88 0 loss:  1.231913447380066\n",
      "88 acc:  0.3043\n",
      "89 0 loss:  1.2175697088241577\n",
      "89 acc:  0.3015\n",
      "90 0 loss:  1.198683500289917\n",
      "90 acc:  0.3001\n",
      "91 0 loss:  1.1913301944732666\n",
      "91 acc:  0.2964\n",
      "92 0 loss:  1.199345588684082\n",
      "92 acc:  0.2954\n",
      "93 0 loss:  1.2079062461853027\n",
      "93 acc:  0.2988\n",
      "94 0 loss:  1.1815135478973389\n",
      "94 acc:  0.3006\n",
      "95 0 loss:  1.1189360618591309\n",
      "95 acc:  0.3027\n",
      "96 0 loss:  1.0444450378417969\n",
      "96 acc:  0.3025\n",
      "97 0 loss:  1.0074117183685303\n",
      "97 acc:  0.3038\n",
      "98 0 loss:  0.9921055436134338\n",
      "98 acc:  0.3052\n",
      "99 0 loss:  0.9761277437210083\n",
      "99 acc:  0.2985\n",
      "100 0 loss:  0.9715079665184021\n",
      "100 acc:  0.3031\n",
      "101 0 loss:  0.982905387878418\n",
      "101 acc:  0.316\n",
      "102 0 loss:  0.9339463114738464\n",
      "102 acc:  0.312\n",
      "103 0 loss:  0.9272086024284363\n",
      "103 acc:  0.3115\n",
      "104 0 loss:  0.890293300151825\n",
      "104 acc:  0.3115\n",
      "105 0 loss:  0.8486891984939575\n",
      "105 acc:  0.3146\n",
      "106 0 loss:  0.8169058561325073\n",
      "106 acc:  0.3147\n",
      "107 0 loss:  0.8135854601860046\n",
      "107 acc:  0.3178\n",
      "108 0 loss:  0.8307024836540222\n",
      "108 acc:  0.3172\n",
      "109 0 loss:  0.8136793375015259\n",
      "109 acc:  0.3171\n",
      "110 0 loss:  0.7855110168457031\n",
      "110 acc:  0.3164\n",
      "111 0 loss:  0.7663869857788086\n",
      "111 acc:  0.3134\n",
      "112 0 loss:  0.7689605951309204\n",
      "112 acc:  0.308\n",
      "113 0 loss:  0.7912368178367615\n",
      "113 acc:  0.3028\n",
      "114 0 loss:  0.8631963729858398\n",
      "114 acc:  0.298\n",
      "115 0 loss:  0.9587278962135315\n",
      "115 acc:  0.3005\n",
      "116 0 loss:  0.8577141761779785\n",
      "116 acc:  0.2988\n",
      "117 0 loss:  0.808154821395874\n",
      "117 acc:  0.2984\n",
      "118 0 loss:  0.7621903419494629\n",
      "118 acc:  0.3064\n",
      "119 0 loss:  0.7346778512001038\n",
      "119 acc:  0.3055\n",
      "120 0 loss:  0.730311930179596\n",
      "120 acc:  0.3025\n",
      "121 0 loss:  0.7193835973739624\n",
      "121 acc:  0.3031\n",
      "122 0 loss:  0.7283860445022583\n",
      "122 acc:  0.3058\n",
      "123 0 loss:  0.6830779314041138\n",
      "123 acc:  0.3063\n",
      "124 0 loss:  0.6499834060668945\n",
      "124 acc:  0.3059\n",
      "125 0 loss:  0.7186907529830933\n",
      "125 acc:  0.2988\n",
      "126 0 loss:  0.7483271360397339\n",
      "126 acc:  0.2999\n",
      "127 0 loss:  0.6587439775466919\n",
      "127 acc:  0.3036\n",
      "128 0 loss:  0.6219865679740906\n",
      "128 acc:  0.298\n",
      "129 0 loss:  0.6374398469924927\n",
      "129 acc:  0.2888\n",
      "130 0 loss:  0.6769800186157227\n",
      "130 acc:  0.2845\n",
      "131 0 loss:  0.714612603187561\n",
      "131 acc:  0.2876\n",
      "132 0 loss:  0.6339818239212036\n",
      "132 acc:  0.2958\n",
      "133 0 loss:  0.5625898838043213\n",
      "133 acc:  0.3004\n",
      "134 0 loss:  0.572247326374054\n",
      "134 acc:  0.3017\n",
      "135 0 loss:  0.6235752701759338\n",
      "135 acc:  0.2962\n",
      "136 0 loss:  0.717674195766449\n",
      "136 acc:  0.2919\n",
      "137 0 loss:  0.7234501838684082\n",
      "137 acc:  0.3016\n",
      "138 0 loss:  0.5972691774368286\n",
      "138 acc:  0.3064\n",
      "139 0 loss:  0.5576895475387573\n",
      "139 acc:  0.308\n",
      "140 0 loss:  0.5964648723602295\n",
      "140 acc:  0.3048\n",
      "141 0 loss:  0.5938917398452759\n",
      "141 acc:  0.3036\n",
      "142 0 loss:  0.6026372909545898\n",
      "142 acc:  0.2986\n",
      "143 0 loss:  0.6271966099739075\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-02fa304c7c80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# compute grades\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m# update variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/tf2_py3/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/tf2_py3/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/environments/tf2_py3/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/tf2_py3/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_FusedBatchNormGrad\u001b[0;34m(op, *grad)\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegisterGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FusedBatchNorm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_FusedBatchNormGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_BaseFusedBatchNormGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/tf2_py3/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_BaseFusedBatchNormGrad\u001b[0;34m(op, use_v2, *grad)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"NHWC\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         is_training=is_training)\n\u001b[0m\u001b[1;32m    890\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"NCHW\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/tf2_py3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mfused_batch_norm_grad\u001b[0;34m(y_backprop, x, scale, reserve_space_1, reserve_space_2, epsilon, data_format, is_training, name)\u001b[0m\n\u001b[1;32m   4046\u001b[0m         \u001b[0;34m\"FusedBatchNormGrad\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4047\u001b[0m         \u001b[0my_backprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreserve_space_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreserve_space_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"epsilon\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4048\u001b[0;31m         epsilon, \"data_format\", data_format, \"is_training\", is_training)\n\u001b[0m\u001b[1;32m   4049\u001b[0m       \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FusedBatchNormGradOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4050\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(500):\n",
    "    for step, (x, y) in enumerate(train_db):\n",
    "        with tf.GradientTape() as tape:   \n",
    "            logits = model(x)   \n",
    "            y_onehot = tf.one_hot(y, depth=100)\n",
    "            # compute loss\n",
    "            loss = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        # compute grades    \n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        # update variables \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        # show loss\n",
    "        if step % 100 == 0:\n",
    "            print(epoch, step, 'loss: ', float(loss))\n",
    "            \n",
    "    # test    \n",
    "    total_num = 0\n",
    "    total_correct = 0\n",
    "    for x, y in test_db:\n",
    "        logits = model(x)\n",
    "        prob = tf.nn.softmax(logits, axis=1)\n",
    "        preb = tf.argmax(prob, axis=1)    # int64\n",
    "        preb = tf.cast(preb, dtype=tf.int32)\n",
    "        \n",
    "        correct = tf.cast(tf.equal(preb, y), dtype=tf.int32)\n",
    "        correct = tf.reduce_sum(correct)\n",
    "        \n",
    "        total_num += x.shape[0]\n",
    "        total_correct += int(correct)\n",
    "        \n",
    "    acc = total_correct / total_num\n",
    "    print(epoch, 'acc: ', acc)\n",
    "    \n",
    "    if epoch % 100 ==0:\n",
    "        # 保存模型\n",
    "        \n",
    "        pass\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

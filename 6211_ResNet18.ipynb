{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential, datasets, optimizers\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "tf.random.set_seed(2345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 BasicBlock类（残差基本单元）\n",
    "class BasicBlock(layers.Layer):\n",
    "    # 其中 filter_num 是该卷积核通道的数量，是把输入进该卷积核的channel转换为数量为filter_num大小\n",
    "    def __init__(self, filter_num, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        # 定义BasicBlock中的第一个卷积单元核，包括3*3的卷积、BachNom、ReLU。\n",
    "        # 这个卷积核接受一个输入数据 [b, h, w, c],\n",
    "        self.conv1 = layers.Conv2D(filter_num, kernel_size=[3, 3], strides=stride, padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.Activation('relu')\n",
    "        # 定义BasicBlock中的第二个卷积单元变量\n",
    "        # 其中strides 设置为1，表示为不间隔进行卷积操作，不减少featureMap\n",
    "        self.conv2 = layers.Conv2D(filter_num, kernel_size=[3, 3], strides=1, padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.relu2 = layers.Activation('relu')\n",
    "        # 定义残差网络中，短接的那一部分；分为stride为1和不为1两种情况，满足最后加和条件\n",
    "        if stride != 1:\n",
    "            self.downsample = Sequential()\n",
    "            self.downsample.add(layers.Conv2D(filter_num, kernel_size=[1, 1], strides=stride))\n",
    "        else:\n",
    "            self.downsample = lambda x : x\n",
    "            \n",
    "    # call() 在调用该类时调用这个call() 方法，实现前向传播，在调用BasicBlock类对象时，执行该方法，\n",
    "    # 第一个参数是输入该神经网络的数据 [b, h, w, c]\n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs [b, h, w, c]\n",
    "        out1 = self.conv1(inputs)\n",
    "        out1 = self.bn1(out1)\n",
    "        out1 = self.relu1(out1)\n",
    "        \n",
    "        out2 = self.conv2(out1)\n",
    "        out2 = self.bn2(out2)\n",
    "        \n",
    "        identity_out = self.downsample(inputs)\n",
    "        output = layers.add([out2, identity_out])    # 调用layers.add() 实现对应元素相加\n",
    "        output = self.relu2(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "# 定义RetNet 类（包括定义残差块build_RestBlock，包括两个基本的残差基本单元）\n",
    "class RetNet(keras.Model):\n",
    "    # 定义初始化函数\n",
    "    def __init__(self, layer_dims, num_classes=100):\n",
    "        super(RetNet, self).__init__()\n",
    "        # 定义预处理的一个卷积层\n",
    "        self.stem = Sequential([\n",
    "            layers.Conv2D(64, kernel_size=[3, 3], strides=1),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation('relu'),\n",
    "            layers.MaxPool2D(pool_size=(2, 2), strides=1, padding='same')\n",
    "        ])\n",
    "        # 创建4个RestBlock，指定每个的channel（filter数量），\n",
    "        # 其中从第二个起设置stride=2，以达到减小featureMap的目的\n",
    "        self.layer1 = self.build_RestBlock(64, layer_dims[0])\n",
    "        self.layer2 = self.build_RestBlock(128, layer_dims[1], stride=2)\n",
    "        self.layer3 = self.build_RestBlock(256, layer_dims[2], stride=2)\n",
    "        self.layer4 = self.build_RestBlock(512, layer_dims[3], stride=2)\n",
    "        # 考虑到全连接层输入决定于前面层的输出 [b, h, w, 512]，需要降维打平操作\n",
    "        # 调用layers.GlobalAveragePooling2D()方法，简称GAP，全局池化层\n",
    "        # 具体可参考：https://www.cnblogs.com/hutao722/p/10008581.html\n",
    "        self.avgpool = layers.GlobalAveragePooling2D()\n",
    "        # 创建全连接层，用来分类,参数是输出类别数\n",
    "        self.fc = layers.Dense(num_classes)\n",
    "    # 前向传播    \n",
    "    def call(self, inputs, training=None):\n",
    "        # 预处理卷积层\n",
    "        x = self.stem(inputs)\n",
    "        # 四个残差块，包括8个残差单元，一个经过16个卷积核\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        # 全池化层 [b, c]\n",
    "        x = self.avgpool(x)\n",
    "        # 全连接层 [b, 100]\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # 定义残差块，包括两个残差基本单元，filter_num该残差块中通道的数量，blocks表示包含的残差基本单元数\n",
    "    def build_RestBlock(self, filter_num, blocks, stride=1):\n",
    "        # 定义神经网络容器Sequential\n",
    "        res_blocks = Sequential()\n",
    "        # 在res_blocks添加层（残差基本单元BasicBlock），可能发生下采样\n",
    "        res_blocks.add(BasicBlock(filter_num, stride))\n",
    "        # \n",
    "        for _ in range(1, blocks):    # 设置为从1到 blocks，其中包括1，但不包括 blocks\n",
    "            # 不允许下采样，因为在残差单元中要保存featureMap不变，便于与identity进行加和操作\n",
    "            res_blocks.add(BasicBlock(filter_num, stride=1))\n",
    "            \n",
    "        return res_blocks\n",
    "    \n",
    "# 定义ResNet18 [2, 2, 2, 2]\n",
    "def resnet18():\n",
    "    return RetNet([2, 2, 2, 2])\n",
    "    \n",
    "# 定义ResNet34 [3, 4, 6, 3]  \n",
    "def resnet34():\n",
    "    return RetNet([3, 4, 6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_train:  (256, 32, 32, 3) (256,) tf.Tensor(-0.5, shape=(), dtype=float32) tf.Tensor(99, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 定义预处理函数\n",
    "def preprocess(x, y):\n",
    "    # [-0.5 ~ 0.5]\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255. - 0.5\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x, y\n",
    "\n",
    "# 加载数据集和数据集预处理\n",
    "(x, y), (x_test, y_test) = datasets.cifar100.load_data()\n",
    "y = tf.squeeze(y, axis=1)\n",
    "y_test = tf.squeeze(y_test, axis=1)\n",
    "\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "train_db = train_db.shuffle(10000).map(preprocess).batch(256)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_db = test_db.map(preprocess).batch(256)\n",
    "\n",
    "# 查看数据集切片\n",
    "sample = next(iter(train_db))\n",
    "print('sample_train: ', sample[0].shape, sample[1].shape, tf.reduce_min(sample[0]),\n",
    "     tf.reduce_max(sample[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ret_net_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_10 (Sequential)   multiple                  2048      \n",
      "_________________________________________________________________\n",
      "sequential_11 (Sequential)   multiple                  148736    \n",
      "_________________________________________________________________\n",
      "sequential_12 (Sequential)   multiple                  526976    \n",
      "_________________________________________________________________\n",
      "sequential_14 (Sequential)   multiple                  2102528   \n",
      "_________________________________________________________________\n",
      "sequential_16 (Sequential)   multiple                  8399360   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  51300     \n",
      "=================================================================\n",
      "Total params: 11,230,948\n",
      "Trainable params: 11,223,140\n",
      "Non-trainable params: 7,808\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 生成ResNet18网络\n",
    "model = resnet18()\n",
    "model.build(input_shape=(None, 32, 32, 3))\n",
    "model.summary()\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optimizers.Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss:  4.603561878204346\n",
      "0 100 loss:  4.31700325012207\n",
      "0 acc:  0.0589\n",
      "1 0 loss:  4.096081733703613\n",
      "1 100 loss:  3.809342384338379\n",
      "1 acc:  0.1451\n",
      "2 0 loss:  3.621166944503784\n",
      "2 100 loss:  3.3795464038848877\n",
      "2 acc:  0.2145\n",
      "3 0 loss:  3.22817325592041\n",
      "3 100 loss:  2.904437780380249\n",
      "3 acc:  0.2602\n",
      "4 0 loss:  2.857774257659912\n",
      "4 100 loss:  2.6984262466430664\n",
      "4 acc:  0.287\n",
      "5 0 loss:  2.5338616371154785\n",
      "5 100 loss:  2.5153427124023438\n",
      "5 acc:  0.3092\n",
      "6 0 loss:  2.3365795612335205\n",
      "6 100 loss:  2.3313212394714355\n",
      "6 acc:  0.3099\n",
      "7 0 loss:  2.1136186122894287\n",
      "7 100 loss:  2.0633597373962402\n",
      "7 acc:  0.317\n",
      "8 0 loss:  1.8272517919540405\n",
      "8 100 loss:  1.9870266914367676\n",
      "8 acc:  0.3022\n",
      "9 0 loss:  1.8123266696929932\n",
      "9 100 loss:  1.8604434728622437\n",
      "9 acc:  0.2904\n",
      "10 0 loss:  1.7031092643737793\n",
      "10 100 loss:  1.7336666584014893\n",
      "10 acc:  0.2994\n",
      "11 0 loss:  1.4488496780395508\n",
      "11 100 loss:  1.583518385887146\n",
      "11 acc:  0.3027\n",
      "12 0 loss:  1.4021451473236084\n",
      "12 100 loss:  1.482235074043274\n",
      "12 acc:  0.2768\n",
      "13 0 loss:  1.3865796327590942\n",
      "13 100 loss:  1.4918891191482544\n",
      "13 acc:  0.2951\n",
      "14 0 loss:  1.0319523811340332\n",
      "14 100 loss:  1.2628560066223145\n",
      "14 acc:  0.3044\n",
      "15 0 loss:  0.8569560647010803\n",
      "15 100 loss:  0.9928116798400879\n",
      "15 acc:  0.2824\n",
      "16 0 loss:  0.7855664491653442\n",
      "16 100 loss:  0.7656699419021606\n",
      "16 acc:  0.2878\n",
      "17 0 loss:  0.6737980842590332\n",
      "17 100 loss:  0.6058724522590637\n",
      "17 acc:  0.2868\n",
      "18 0 loss:  0.6898605227470398\n",
      "18 100 loss:  0.5543966293334961\n",
      "18 acc:  0.2786\n",
      "19 0 loss:  0.6850006580352783\n",
      "19 100 loss:  0.31736910343170166\n",
      "19 acc:  0.2961\n",
      "20 0 loss:  0.4172094464302063\n",
      "20 100 loss:  0.3630528450012207\n",
      "20 acc:  0.3072\n",
      "21 0 loss:  0.37011465430259705\n",
      "21 100 loss:  0.34279748797416687\n",
      "21 acc:  0.3147\n",
      "22 0 loss:  0.2557314932346344\n",
      "22 100 loss:  0.3796447813510895\n",
      "22 acc:  0.3156\n",
      "23 0 loss:  0.19416004419326782\n",
      "23 100 loss:  0.24751834571361542\n",
      "23 acc:  0.321\n",
      "24 0 loss:  0.188748300075531\n",
      "24 100 loss:  0.20053225755691528\n",
      "24 acc:  0.3173\n",
      "25 0 loss:  0.139143168926239\n",
      "25 100 loss:  0.2436802089214325\n",
      "25 acc:  0.3174\n",
      "26 0 loss:  0.1672048419713974\n",
      "26 100 loss:  0.21442170441150665\n",
      "26 acc:  0.318\n",
      "27 0 loss:  0.11075770854949951\n",
      "27 100 loss:  0.10534272342920303\n",
      "27 acc:  0.324\n",
      "28 0 loss:  0.04233524575829506\n",
      "28 100 loss:  0.2026204615831375\n",
      "28 acc:  0.326\n",
      "29 0 loss:  0.07057671993970871\n",
      "29 100 loss:  0.07269904762506485\n",
      "29 acc:  0.3274\n",
      "30 0 loss:  0.11796090006828308\n",
      "30 100 loss:  0.14916376769542694\n",
      "30 acc:  0.3277\n",
      "31 0 loss:  0.13769444823265076\n",
      "31 100 loss:  0.1071469634771347\n",
      "31 acc:  0.3261\n",
      "32 0 loss:  0.16899055242538452\n",
      "32 100 loss:  0.12107168883085251\n",
      "32 acc:  0.3232\n",
      "33 0 loss:  0.10539881139993668\n",
      "33 100 loss:  0.08830653131008148\n",
      "33 acc:  0.3194\n",
      "34 0 loss:  0.10151354968547821\n",
      "34 100 loss:  0.0444108210504055\n",
      "34 acc:  0.3285\n",
      "35 0 loss:  0.044017449021339417\n",
      "35 100 loss:  0.12196115404367447\n",
      "35 acc:  0.3263\n",
      "36 0 loss:  0.12328377366065979\n",
      "36 100 loss:  0.08082712441682816\n",
      "36 acc:  0.3267\n",
      "37 0 loss:  0.10037574917078018\n",
      "37 100 loss:  0.15598395466804504\n",
      "37 acc:  0.3218\n",
      "38 0 loss:  0.10350456088781357\n",
      "38 100 loss:  0.07156626880168915\n",
      "38 acc:  0.3273\n",
      "39 0 loss:  0.04362037032842636\n",
      "39 100 loss:  0.09627995640039444\n",
      "39 acc:  0.3327\n",
      "40 0 loss:  0.043417688459157944\n",
      "40 100 loss:  0.04369957000017166\n",
      "40 acc:  0.3166\n",
      "41 0 loss:  0.08362288028001785\n",
      "41 100 loss:  0.06173792481422424\n",
      "41 acc:  0.3261\n",
      "42 0 loss:  0.052226535975933075\n",
      "42 100 loss:  0.041179411113262177\n",
      "42 acc:  0.3296\n",
      "43 0 loss:  0.11624584347009659\n",
      "43 100 loss:  0.15742959082126617\n",
      "43 acc:  0.3271\n",
      "44 0 loss:  0.1807127743959427\n",
      "44 100 loss:  0.05340476334095001\n",
      "44 acc:  0.3223\n",
      "45 0 loss:  0.1404697746038437\n",
      "45 100 loss:  0.11661568284034729\n",
      "45 acc:  0.3305\n",
      "46 0 loss:  0.1116817444562912\n",
      "46 100 loss:  0.12771263718605042\n",
      "46 acc:  0.3253\n",
      "47 0 loss:  0.04776057228446007\n",
      "47 100 loss:  0.07152953743934631\n",
      "47 acc:  0.3364\n",
      "48 0 loss:  0.07931064814329147\n",
      "48 100 loss:  0.0653391033411026\n",
      "48 acc:  0.3273\n",
      "49 0 loss:  0.09457956254482269\n",
      "49 100 loss:  0.03236173465847969\n",
      "49 acc:  0.3318\n",
      "50 0 loss:  0.0878724530339241\n",
      "50 100 loss:  0.07539229840040207\n",
      "50 acc:  0.3322\n",
      "51 0 loss:  0.0149777140468359\n",
      "51 100 loss:  0.0994996502995491\n",
      "51 acc:  0.3338\n",
      "52 0 loss:  0.07515370845794678\n",
      "52 100 loss:  0.12568940222263336\n",
      "52 acc:  0.3304\n",
      "53 0 loss:  0.08609061688184738\n",
      "53 100 loss:  0.05070187896490097\n",
      "53 acc:  0.3321\n",
      "54 0 loss:  0.1037098616361618\n",
      "54 100 loss:  0.12290734052658081\n",
      "54 acc:  0.3341\n",
      "55 0 loss:  0.04460006207227707\n",
      "55 100 loss:  0.04647357016801834\n",
      "55 acc:  0.3302\n",
      "56 0 loss:  0.06241230666637421\n",
      "56 100 loss:  0.06524384021759033\n",
      "56 acc:  0.3328\n",
      "57 0 loss:  0.05712540075182915\n",
      "57 100 loss:  0.06242816150188446\n",
      "57 acc:  0.3276\n",
      "58 0 loss:  0.07491093128919601\n",
      "58 100 loss:  0.10091696679592133\n",
      "58 acc:  0.3321\n",
      "59 0 loss:  0.03990783542394638\n",
      "59 100 loss:  0.15275581181049347\n",
      "59 acc:  0.3334\n",
      "60 0 loss:  0.027981342747807503\n",
      "60 100 loss:  0.055529411882162094\n",
      "60 acc:  0.3284\n",
      "61 0 loss:  0.086992546916008\n",
      "61 100 loss:  0.09750504791736603\n",
      "61 acc:  0.3258\n",
      "62 0 loss:  0.06076424568891525\n",
      "62 100 loss:  0.048137836158275604\n",
      "62 acc:  0.3324\n",
      "63 0 loss:  0.03841220960021019\n",
      "63 100 loss:  0.0664154589176178\n",
      "63 acc:  0.326\n",
      "64 0 loss:  0.09117338061332703\n",
      "64 100 loss:  0.06575703620910645\n",
      "64 acc:  0.3238\n",
      "65 0 loss:  0.01036648266017437\n",
      "65 100 loss:  0.05932243913412094\n",
      "65 acc:  0.3256\n",
      "66 0 loss:  0.09265246987342834\n",
      "66 100 loss:  0.05544911324977875\n",
      "66 acc:  0.3261\n",
      "67 0 loss:  0.16791149973869324\n",
      "67 100 loss:  0.08971168845891953\n",
      "67 acc:  0.3263\n",
      "68 0 loss:  0.06926265358924866\n",
      "68 100 loss:  0.05126672983169556\n",
      "68 acc:  0.3268\n",
      "69 0 loss:  0.05991451442241669\n",
      "69 100 loss:  0.02606012672185898\n",
      "69 acc:  0.3344\n",
      "70 0 loss:  0.026898784562945366\n",
      "70 100 loss:  0.06980542838573456\n",
      "70 acc:  0.3274\n",
      "71 0 loss:  0.015615954995155334\n",
      "71 100 loss:  0.03306568041443825\n",
      "71 acc:  0.3323\n",
      "72 0 loss:  0.016248123720288277\n",
      "72 100 loss:  0.052043840289115906\n",
      "72 acc:  0.3215\n",
      "73 0 loss:  0.0716186910867691\n",
      "73 100 loss:  0.061973121017217636\n",
      "73 acc:  0.3311\n",
      "74 0 loss:  0.026782900094985962\n",
      "74 100 loss:  0.08874516934156418\n",
      "74 acc:  0.3251\n",
      "75 0 loss:  0.024485811591148376\n",
      "75 100 loss:  0.052006494253873825\n",
      "75 acc:  0.3327\n",
      "76 0 loss:  0.01244952343404293\n",
      "76 100 loss:  0.07706806808710098\n",
      "76 acc:  0.3312\n",
      "77 0 loss:  0.08555597066879272\n",
      "77 100 loss:  0.04415242746472359\n",
      "77 acc:  0.3271\n",
      "78 0 loss:  0.08077028393745422\n",
      "78 100 loss:  0.04339425638318062\n",
      "78 acc:  0.3349\n",
      "79 0 loss:  0.08272112905979156\n",
      "79 100 loss:  0.032049357891082764\n",
      "79 acc:  0.3312\n",
      "80 0 loss:  0.043402448296546936\n",
      "80 100 loss:  0.04525238648056984\n",
      "80 acc:  0.3324\n",
      "81 0 loss:  0.10040485858917236\n",
      "81 100 loss:  0.0630616843700409\n",
      "81 acc:  0.3299\n",
      "82 0 loss:  0.04338717460632324\n",
      "82 100 loss:  0.0601530447602272\n",
      "82 acc:  0.3292\n",
      "83 0 loss:  0.0424201563000679\n",
      "83 100 loss:  0.12632104754447937\n",
      "83 acc:  0.3339\n",
      "84 0 loss:  0.021520566195249557\n",
      "84 100 loss:  0.05427521839737892\n",
      "84 acc:  0.3333\n",
      "85 0 loss:  0.062293801456689835\n",
      "85 100 loss:  0.05248738452792168\n",
      "85 acc:  0.3287\n",
      "86 0 loss:  0.02925201691687107\n",
      "86 100 loss:  0.09218509495258331\n",
      "86 acc:  0.321\n",
      "87 0 loss:  0.0578543096780777\n",
      "87 100 loss:  0.11598518490791321\n",
      "87 acc:  0.3342\n",
      "88 0 loss:  0.015457685105502605\n",
      "88 100 loss:  0.017145883291959763\n",
      "88 acc:  0.3236\n",
      "89 0 loss:  0.02782129868865013\n",
      "89 100 loss:  0.036112844944000244\n",
      "89 acc:  0.3325\n",
      "90 0 loss:  0.01609591208398342\n",
      "90 100 loss:  0.1059245616197586\n",
      "90 acc:  0.3334\n",
      "91 0 loss:  0.019665518775582314\n",
      "91 100 loss:  0.08244605362415314\n",
      "91 acc:  0.3321\n",
      "92 0 loss:  0.008548235520720482\n",
      "92 100 loss:  0.05753109231591225\n",
      "92 acc:  0.3238\n",
      "93 0 loss:  0.04749009758234024\n",
      "93 100 loss:  0.13085651397705078\n",
      "93 acc:  0.3239\n",
      "94 0 loss:  0.05959567055106163\n",
      "94 100 loss:  0.06697112321853638\n",
      "94 acc:  0.3267\n",
      "95 0 loss:  0.24232514202594757\n",
      "95 100 loss:  0.07395699620246887\n",
      "95 acc:  0.3344\n",
      "96 0 loss:  0.08146737515926361\n",
      "96 100 loss:  0.10657135397195816\n",
      "96 acc:  0.3328\n",
      "97 0 loss:  0.017816536128520966\n",
      "97 100 loss:  0.028818707913160324\n",
      "97 acc:  0.3358\n",
      "98 0 loss:  0.05594591796398163\n",
      "98 100 loss:  0.09181632846593857\n",
      "98 acc:  0.3413\n",
      "99 0 loss:  0.03845074400305748\n",
      "99 100 loss:  0.0158068984746933\n",
      "99 acc:  0.3372\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(100):\n",
    "    for step, (x, y) in enumerate(train_db):\n",
    "        with tf.GradientTape() as tape:   \n",
    "            logits = model(x)   \n",
    "            y_onehot = tf.one_hot(y, depth=500)\n",
    "            # compute loss\n",
    "            loss = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        # compute grades    \n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        # update variables \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        # show loss\n",
    "        if step % 100 == 0:\n",
    "            print(epoch, step, 'loss: ', float(loss))\n",
    "            \n",
    "    # test    \n",
    "    total_num = 0\n",
    "    total_correct = 0\n",
    "    for x, y in test_db:\n",
    "        logits = model(x)\n",
    "        prob = tf.nn.softmax(logits, axis=1)\n",
    "        preb = tf.argmax(prob, axis=1)    # int64\n",
    "        preb = tf.cast(preb, dtype=tf.int32)\n",
    "        \n",
    "        correct = tf.cast(tf.equal(preb, y), dtype=tf.int32)\n",
    "        correct = tf.reduce_sum(correct)\n",
    "        \n",
    "        total_num += x.shape[0]\n",
    "        total_correct += int(correct)\n",
    "        \n",
    "    acc = total_correct / total_num\n",
    "    print(epoch, 'acc: ', acc)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

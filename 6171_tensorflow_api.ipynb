{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow_API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, optimizers, Sequential,metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsz = 128\n",
    "(x, y), (x_val, y_val) = datasets.fashion_mnist.load_data()\n",
    "db = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "db = db.map(preprocess).shuffle(60000).batch(batchsz).repeat(10)\n",
    "\n",
    "db_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "db_val = db_val.map(preprocess).batch(batchsz, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Sequential([   \n",
    "    layers.Dense(256, activation=tf.nn.relu),    # [b, 784] => [b, 256]\n",
    "    layers.Dense(128, activation=tf.nn.relu),    # [b, 256] => [b, 128]\n",
    "    layers.Dense(64, activation=tf.nn.relu),     # [b, 128] => [b, 64]\n",
    "    layers.Dense(32, activation=tf.nn.relu),     # [b, 64] => [b, 32]\n",
    "    layers.Dense(10)                             # [b, 32] => [b, 10]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  200960    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  2080      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  330       \n",
      "=================================================================\n",
      "Total params: 244,522\n",
      "Trainable params: 244,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "0 loss: 2.3432314\n",
      "77 Evaluate Acc:  0.14022435897435898 0.14022435\n",
      "100 loss: 0.86405975\n",
      "200 loss: 0.5323111\n",
      "300 loss: 0.46110824\n",
      "400 loss: 0.4392525\n",
      "500 loss: 0.41368496\n",
      "77 Evaluate Acc:  0.8434495192307693 0.84344953\n",
      "600 loss: 0.39177987\n",
      "700 loss: 0.38740557\n",
      "800 loss: 0.373268\n",
      "900 loss: 0.36900088\n",
      "1000 loss: 0.3363058\n",
      "77 Evaluate Acc:  0.8568709935897436 0.856871\n",
      "1100 loss: 0.3449173\n",
      "1200 loss: 0.3478583\n",
      "1300 loss: 0.3376\n",
      "1400 loss: 0.32909498\n",
      "1500 loss: 0.31012326\n",
      "77 Evaluate Acc:  0.8692908653846154 0.8692909\n",
      "1600 loss: 0.3120491\n",
      "1700 loss: 0.3178554\n",
      "1800 loss: 0.31570438\n",
      "1900 loss: 0.3001312\n",
      "2000 loss: 0.2982847\n",
      "77 Evaluate Acc:  0.8653846153846154 0.86538464\n",
      "2100 loss: 0.28984258\n",
      "2200 loss: 0.2908243\n",
      "2300 loss: 0.3162796\n",
      "2400 loss: 0.28500426\n",
      "2500 loss: 0.2791392\n",
      "77 Evaluate Acc:  0.8723958333333334 0.8723958\n",
      "2600 loss: 0.28061727\n",
      "2700 loss: 0.27758157\n",
      "2800 loss: 0.27904463\n",
      "2900 loss: 0.26292363\n",
      "3000 loss: 0.26692402\n",
      "77 Evaluate Acc:  0.8772035256410257 0.8772035\n",
      "3100 loss: 0.26576436\n",
      "3200 loss: 0.2549018\n",
      "3300 loss: 0.2665629\n",
      "3400 loss: 0.249354\n",
      "3500 loss: 0.25948066\n",
      "77 Evaluate Acc:  0.8808092948717948 0.8808093\n",
      "3600 loss: 0.24821158\n",
      "3700 loss: 0.26204345\n",
      "3800 loss: 0.2587381\n",
      "3900 loss: 0.23606588\n",
      "4000 loss: 0.24521199\n",
      "77 Evaluate Acc:  0.8865184294871795 0.8865184\n",
      "4100 loss: 0.2513172\n",
      "4200 loss: 0.24791886\n",
      "4300 loss: 0.23512886\n",
      "4400 loss: 0.23768364\n",
      "4500 loss: 0.23645917\n",
      "77 Evaluate Acc:  0.8843149038461539 0.8843149\n",
      "4600 loss: 0.23340265\n"
     ]
    }
   ],
   "source": [
    "network.build(input_shape=(None, 28*28))\n",
    "network.summary()\n",
    "\n",
    "optimizer = optimizers.Adam(lr=0.001)\n",
    "\n",
    "# [__1]创建metrics，用于求精确度\n",
    "acc_meter = metrics.Accuracy()\n",
    "# [1]创建metrics，用于求平均值\n",
    "loss_meter = metrics.Mean()\n",
    "\n",
    "for step, (x, y) in enumerate(db):\n",
    "    with tf.GradientTape() as tape:\n",
    "        x = tf.reshape(x, (-1, 28*28))\n",
    "        out = network(x)\n",
    "        y_onehot = tf.one_hot(y, depth=10)\n",
    "        loss = tf.reduce_mean(tf.losses.categorical_crossentropy(y_onehot, out, from_logits=True))\n",
    "        \n",
    "        # [2]更新loss_meter 中的数据，添加数据\n",
    "        loss_meter.update_state(loss)\n",
    "                \n",
    "    grads = tape.gradient(loss, network.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        # [3]获取loss_meter的值（loss的没100个的平均值），转换为numpy类型并打印输出\n",
    "        print(step, 'loss:', loss_meter.result().numpy())\n",
    "        # [4]清除所创建的loss_meter的buffer\n",
    "        loss_meter.reset_states()\n",
    "        \n",
    "    # evaluate测试，每500步计算一次精度\n",
    "    if step % 500 == 0:\n",
    "        total, total_correct =0., 0.\n",
    "        # [__2]需要先清空acc_meter\n",
    "        acc_meter.reset_states()\n",
    "        \n",
    "        for step, (x, y) in enumerate(db_val):\n",
    "            x = tf.reshape(x, [-1, 28*28])\n",
    "            out = network(x)\n",
    "            pred = tf.cast(tf.argmax(out, axis=1),tf.int32)\n",
    "            correct = tf.equal(pred, y)\n",
    "            total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()\n",
    "            total += x.shape[0]\n",
    "            # [__3]更新acc_meter中的数据,添加数据\n",
    "            acc_meter.update_state(y, pred)\n",
    "        # [__4]获得acc_meter中的数据，并转化为numpy类型，最后打印输出    \n",
    "        print(step,'Evaluate Acc: ', total_correct/total, acc_meter.result().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

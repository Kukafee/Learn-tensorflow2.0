{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.2827 - accuracy: 0.8368\n",
      "Epoch 2/3\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1388 - accuracy: 0.9576 - val_loss: 0.1470 - val_accuracy: 0.9622\n",
      "Epoch 3/3\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1168 - accuracy: 0.9662\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 0.1222 - accuracy: 0.9697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12223819872663545, 0.9697]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"x is a simple image, not a batch\"\"\"\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    x = tf.reshape(x, [28*28])\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    y = tf.one_hot(y, depth=10)\n",
    "    return x, y\n",
    "\n",
    "batchsz = 128\n",
    "(x, y), (x_val, y_val) = datasets.mnist.load_data()\n",
    "# print('datasets:', x.shape, y.shape, x.min(), y.min())\n",
    "\n",
    "db = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "db = db.map(preprocess).shuffle(60000).batch(batchsz)\n",
    "ds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "ds_val = ds_val.map(preprocess).batch(batchsz)\n",
    "\n",
    "class MyDense(layers.Layer):\n",
    "    def __init__(self, inp_dim, outp_dim):\n",
    "        super(MyDense, self).__init__()\n",
    "        self.kernel = self.add_variable('w',[inp_dim, outp_dim])\n",
    "        self.bias = self.add_variable('b', [outp_dim])\n",
    "    def call(self, inputs, training=None):\n",
    "        out = inputs @ self.kernel + self.bias\n",
    "        return out\n",
    "    \n",
    "class MyModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = MyDense(28*28, 256)\n",
    "        self.fc2 = MyDense(256, 128)\n",
    "        self.fc3 = MyDense(128, 64)\n",
    "        self.fc4 = MyDense(64, 32)\n",
    "        self.fc5 = MyDense(32, 10)\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.fc1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# 创建神经网络 network\n",
    "network = MyModel()\n",
    "network.compile(optimizer=optimizers.Adam(lr=0.01),\n",
    "               loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
    "               metrics=['accuracy'])\n",
    "network.fit(db, epochs=3, validation_data=ds_val, validation_freq=2)\n",
    "network.evaluate(ds_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存 network 的参数到文件weights1.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved weights !\n",
      "Loaded weights !\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.1298 - accuracy: 0.9582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12983709347251615, 0.9636]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 保存 network 的******参数******到文件\n",
    "network.save_weights('/home/kukafee/workspace/save_model/weights1.ckpt')\n",
    "# 打印信息\n",
    "print('Saved weights !')\n",
    "# 删除网络\n",
    "del network\n",
    "# 重新创建神经网络\n",
    "network = MyModel()\n",
    "network.compile(optimizer=optimizers.Adam(lr=0.01),\n",
    "               loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
    "               metrics=['accuracy'])\n",
    "# 加载文件中的数据（保存有链接权重的数据）\n",
    "network.load_weights('/home/kukafee/workspace/save_model/weights1.ckpt')\n",
    "# 打印信息\n",
    "print('Loaded weights !')\n",
    "network.evaluate(ds_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存 network1 的所有信息到文件model.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_45 (Dense)             multiple                  200960    \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             multiple                  32896     \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             multiple                  8256      \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             multiple                  2080      \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             multiple                  330       \n",
      "=================================================================\n",
      "Total params: 244,522\n",
      "Trainable params: 244,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.2762 - accuracy: 0.8376\n",
      "Epoch 2/3\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1353 - accuracy: 0.9579 - val_loss: 0.1143 - val_accuracy: 0.9671\n",
      "Epoch 3/3\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1124 - accuracy: 0.9664\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.1248 - accuracy: 0.9699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0617 16:01:14.101134 140530799548224 hdf5_format.py:266] Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved total model !\n",
      "Load model from file:\n",
      "Loaded the model from file !\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown entry in loss dictionary: class_name. Only expected following keys: ['output_1']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ff28c9c2bf45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mnetwork1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/environments/tf2_py3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steps'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/tf2_py3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2556\u001b[0m             \u001b[0mloss_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m             \u001b[0mtarget_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2558\u001b[0;31m             run_eagerly=self.run_eagerly)\n\u001b[0m\u001b[1;32m   2559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2560\u001b[0m     \u001b[0;31m# In graph mode, if we had just set inputs and targets as symbolic tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/tf2_py3/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/tf2_py3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;31m# Prepare list of loss functions, same size of model outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     self.loss_functions = training_utils.prepare_loss_functions(\n\u001b[0;32m--> 304\u001b[0;31m         loss, self.output_names)\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/tf2_py3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mprepare_loss_functions\u001b[0;34m(loss, output_names)\u001b[0m\n\u001b[1;32m   1143\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m         raise ValueError('Unknown entry in loss dictionary: {}. Only expected '\n\u001b[0;32m-> 1145\u001b[0;31m                          'following keys: {}'.format(name, output_names))\n\u001b[0m\u001b[1;32m   1146\u001b[0m     \u001b[0mloss_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown entry in loss dictionary: class_name. Only expected following keys: ['output_1']"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"x is a simple image, not a batch\"\"\"\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    x = tf.reshape(x, [28*28])\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    y = tf.one_hot(y, depth=10)\n",
    "    return x, y\n",
    "\n",
    "batchsz = 128\n",
    "(x, y), (x_val, y_val) = datasets.mnist.load_data()\n",
    "# print('datasets:', x.shape, y.shape, x.min(), y.min())\n",
    "\n",
    "db = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "db = db.map(preprocess).shuffle(60000).batch(batchsz)\n",
    "ds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "ds_val = ds_val.map(preprocess).batch(batchsz)\n",
    "\n",
    "network1 = Sequential([   \n",
    "    layers.Dense(256, activation=tf.nn.relu),    # [b, 784] => [b, 256]\n",
    "    layers.Dense(128, activation=tf.nn.relu),    # [b, 256] => [b, 128]\n",
    "    layers.Dense(64, activation=tf.nn.relu),     # [b, 128] => [b, 64]\n",
    "    layers.Dense(32, activation=tf.nn.relu),     # [b, 64] => [b, 32]\n",
    "    layers.Dense(10)                             # [b, 32] => [b, 10]\n",
    "])\n",
    "network1.build(input_shape=(None, 28*28))\n",
    "network1.summary()\n",
    "# 创建神经网络 network\n",
    "network1.compile(optimizer=optimizers.Adam(lr=0.01),\n",
    "               loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
    "               metrics=['accuracy'])\n",
    "network1.fit(db, epochs=3, validation_data=ds_val, validation_freq=2)\n",
    "network1.evaluate(ds_val)\n",
    "\n",
    "\n",
    "# 保存 network 的******所有信息******到文件\n",
    "network1.save('/home/kukafee/workspace/save_model/model.h5')\n",
    "print('Saved total model !')\n",
    "del network1\n",
    "print('Load model from file:')\n",
    "network1 = tf.keras.models.load_model('/home/kukafee/workspace/save_model/model.h5')\n",
    "print('Loaded the model from file !')\n",
    "# network1.summary()\n",
    "# ????????????????????????????????????\n",
    "x_val = tf.cast(x_val, dtype=tf.float32) / 255.\n",
    "x_val = tf.reshape(x_val, [-1, 28*28])\n",
    "y_val = tf.cast(y_val, dtype=tf.int32)\n",
    "y_val = tf.one_hot(y_val, depth=10)\n",
    "network1.evaluate(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

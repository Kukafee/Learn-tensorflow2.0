{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "### CIFAR100_VGG13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, Sequential, datasets, optimizers\n",
    "import os\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用allow_growth option，刚一开始分配少量的GPU容量，然后按需慢慢的增加，动态申请显存\n",
    "# 由于不会释放内存，所以会导致碎片\n",
    "config = ConfigProto()   # 用在创建session的时候，用来对session进行参数配置\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = tf.Session(config=config)\n",
    "\n",
    "# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "# sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "tf.random.set_seed(2345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution layers\n",
    "conv_layers = [\n",
    "    # 5 units of conv + max poling\n",
    "    # unit 1 \n",
    "    # Conv2D()中第一个参数表示经过这一层的卷积操作后输出的通道数\n",
    "    layers.Conv2D(64, kernel_size=[3, 3], padding='same', activation=tf.nn.relu),\n",
    "    layers.Conv2D(64, kernel_size=[3, 3], padding='same', activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same'),\n",
    "    # unit 2\n",
    "    layers.Conv2D(128, kernel_size=[3, 3], padding='same', activation=tf.nn.relu),\n",
    "    layers.Conv2D(128, kernel_size=[3, 3], padding='same', activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same'),\n",
    "    # unit 3\n",
    "    layers.Conv2D(256, kernel_size=[3, 3], padding='same', activation=tf.nn.relu),\n",
    "    layers.Conv2D(256, kernel_size=[3, 3], padding='same', activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same'),\n",
    "    # unit 4\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding='same', activation=tf.nn.relu),\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding='same', activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same'),\n",
    "    # unit 5\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding='same', activation=tf.nn.relu),\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding='same', activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same')\n",
    "    \n",
    "]\n",
    "conv_net = Sequential(conv_layers)\n",
    "\n",
    "# full connection layers\n",
    "fc_net = Sequential([\n",
    "    layers.Dense(512, activation=tf.nn.relu),    # [b, 512] => [b, 512]\n",
    "    layers.Dense(256, activation=tf.nn.relu),    # [b, 512] => [b, 256]\n",
    "    layers.Dense(100, activation=None)           # [b, 256] => [b, 100]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义预处理函数\n",
    "def preprocess(x, y):\n",
    "    # [0~1]\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x, y\n",
    "# 创建网络层\n",
    "conv_net.build(input_shape=[None, 32, 32, 3])\n",
    "fc_net.build(input_shape=[None, 512])\n",
    "# 定义优化器\n",
    "optimizer = optimizers.Adam(lr=1e-4)\n",
    "# 定义网络成参数列表\n",
    "variables = conv_net.trainable_variables + fc_net.trainable_variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_train:  (256, 32, 32, 3) (256,) tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(99, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集和数据集预处理\n",
    "(x, y), (x_test, y_test) = datasets.cifar100.load_data()\n",
    "y = tf.squeeze(y, axis=1)\n",
    "y_test = tf.squeeze(y_test, axis=1)\n",
    "\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "train_db = train_db.shuffle(10000).map(preprocess).batch(256)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_db = test_db.map(preprocess).batch(256)\n",
    "\n",
    "# 查看数据集切片\n",
    "sample = next(iter(train_db))\n",
    "print('sample_train: ', sample[0].shape, sample[1].shape, tf.reduce_min(sample[0]),\n",
    "     tf.reduce_max(sample[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss:  4.605621814727783\n",
      "0 100 loss:  4.398139953613281\n",
      "0 acc:  0.054\n",
      "1 0 loss:  4.107680320739746\n",
      "1 100 loss:  4.153841018676758\n",
      "1 acc:  0.0811\n",
      "2 0 loss:  3.950089931488037\n",
      "2 100 loss:  3.9207491874694824\n",
      "2 acc:  0.1235\n",
      "3 0 loss:  3.746234178543091\n",
      "3 100 loss:  3.6879868507385254\n",
      "3 acc:  0.1596\n",
      "4 0 loss:  3.539954423904419\n",
      "4 100 loss:  3.4683966636657715\n",
      "4 acc:  0.1824\n",
      "5 0 loss:  3.3869028091430664\n",
      "5 100 loss:  3.3026139736175537\n",
      "5 acc:  0.2104\n",
      "6 0 loss:  3.1666555404663086\n",
      "6 100 loss:  3.1883835792541504\n",
      "6 acc:  0.2304\n",
      "7 0 loss:  2.9812355041503906\n",
      "7 100 loss:  3.0427980422973633\n",
      "7 acc:  0.2393\n",
      "8 0 loss:  2.8564555644989014\n",
      "8 100 loss:  2.9154019355773926\n",
      "8 acc:  0.241\n",
      "9 0 loss:  2.7748284339904785\n",
      "9 100 loss:  2.783421754837036\n",
      "9 acc:  0.2413\n",
      "10 0 loss:  2.6757168769836426\n",
      "10 100 loss:  2.6323559284210205\n",
      "10 acc:  0.2473\n",
      "11 0 loss:  2.572598695755005\n",
      "11 100 loss:  2.4826223850250244\n",
      "11 acc:  0.2516\n",
      "12 0 loss:  2.5238289833068848\n",
      "12 100 loss:  2.41087007522583\n",
      "12 acc:  0.2573\n",
      "13 0 loss:  2.382537364959717\n",
      "13 100 loss:  2.3811721801757812\n",
      "13 acc:  0.2538\n",
      "14 0 loss:  2.282280921936035\n",
      "14 100 loss:  2.3180394172668457\n",
      "14 acc:  0.25\n",
      "15 0 loss:  2.30798077583313\n",
      "15 100 loss:  2.07476806640625\n",
      "15 acc:  0.2469\n",
      "16 0 loss:  2.20932936668396\n",
      "16 100 loss:  1.9535386562347412\n",
      "16 acc:  0.255\n",
      "17 0 loss:  2.199398994445801\n",
      "17 100 loss:  1.9690343141555786\n",
      "17 acc:  0.2428\n",
      "18 0 loss:  1.8688671588897705\n",
      "18 100 loss:  1.8454477787017822\n",
      "18 acc:  0.2397\n",
      "19 0 loss:  1.5712357759475708\n",
      "19 100 loss:  1.7592356204986572\n",
      "19 acc:  0.2554\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    for step, (x, y) in enumerate(train_db):\n",
    "        with tf.GradientTape() as tape:\n",
    "            out = conv_net(x)    # [b, 32, 32, 32, 3] => [b, 1, 1, 512]\n",
    "            out = tf.reshape(out, [-1, 512])    # [b, 1, 1, 512] => [b, 512]\n",
    "            logits = fc_net(out)    # [b, 512] => [b, 100]\n",
    "            y_onehot = tf.one_hot(y, depth=100)\n",
    "            # compute loss\n",
    "            loss = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        # compute grades    \n",
    "        grads = tape.gradient(loss, variables)\n",
    "        # update variables \n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        \n",
    "        # show loss\n",
    "        if step % 100 == 0:\n",
    "            print(epoch, step, 'loss: ', float(loss))\n",
    "            \n",
    "    # test    \n",
    "    total_num = 0\n",
    "    total_correct = 0\n",
    "    for x, y in test_db:\n",
    "        out = conv_net(x)\n",
    "        out = tf.reshape(out, [-1, 512])\n",
    "        logits = fc_net(out)\n",
    "        prob = tf.nn.softmax(logits, axis=1)\n",
    "        preb = tf.argmax(prob, axis=1)    # int64\n",
    "        preb = tf.cast(preb, dtype=tf.int32)\n",
    "        \n",
    "        correct = tf.cast(tf.equal(preb, y), dtype=tf.int32)\n",
    "        correct = tf.reduce_sum(correct)\n",
    "        \n",
    "        total_num += x.shape[0]\n",
    "        total_correct += int(correct)\n",
    "        \n",
    "    acc = total_correct / total_num\n",
    "    print(epoch, 'acc: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
